{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0557cdd915f842a895cd55480e828372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77d35bd47c1c46d29183dbc8dfdc2698",
              "IPY_MODEL_e0d879acc5484a5d8c8294c3a146341d",
              "IPY_MODEL_59169a5324e6466387734854d7b99666"
            ],
            "layout": "IPY_MODEL_2c570e7f9b3a44bf8e439f84c70afc13"
          }
        },
        "77d35bd47c1c46d29183dbc8dfdc2698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0deadc2bf204b82a77fbff52a39c571",
            "placeholder": "​",
            "style": "IPY_MODEL_6186545c6bf740e185bdd919d74604c3",
            "value": "Map: 100%"
          }
        },
        "e0d879acc5484a5d8c8294c3a146341d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_519c5de033e04b7cb73bad43edbfac9b",
            "max": 790,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_755bf30c868349448a23018d5c99200d",
            "value": 790
          }
        },
        "59169a5324e6466387734854d7b99666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b7590a66b7242a189afc21dff8ab090",
            "placeholder": "​",
            "style": "IPY_MODEL_13c592573bec4fdba3e774f3b8fde603",
            "value": " 790/790 [02:29&lt;00:00, 31.80 examples/s]"
          }
        },
        "2c570e7f9b3a44bf8e439f84c70afc13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0deadc2bf204b82a77fbff52a39c571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6186545c6bf740e185bdd919d74604c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "519c5de033e04b7cb73bad43edbfac9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "755bf30c868349448a23018d5c99200d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b7590a66b7242a189afc21dff8ab090": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13c592573bec4fdba3e774f3b8fde603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate datasets peft evaluate bitsandbytes optuna packaging ninja transformers requests --upgrade\n",
        "!pip install transformers --upgrade\n",
        "# !MAX_JOBS=2 pip install flash-attn --no-build-isolation torch-xla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0oWDSrUWjne",
        "outputId": "b0c8f1cf-221b-4a9e-d961-2a8e0035cc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.1)\n",
            "Collecting packaging\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.24.7)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
            "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Downloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.8/362.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja, xxhash, packaging, Mako, fsspec, dill, colorlog, multiprocess, alembic, tokenizers, optuna, bitsandbytes, accelerate, transformers, datasets, evaluate\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.34.2\n",
            "    Uninstalling accelerate-0.34.2:\n",
            "      Successfully uninstalled accelerate-0.34.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.6 accelerate-1.1.1 alembic-1.14.0 bitsandbytes-0.44.1 colorlog-6.9.0 datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 ninja-1.11.1.1 optuna-4.0.0 packaging-24.2 tokenizers-0.20.3 transformers-4.46.2 xxhash-3.5.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import pathlib\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "os.environ['WANDB_MODE'] = 'disabled'\n",
        "drive.mount('/content/drive')\n",
        "# https://cf020031308.github.io/wiki/optuna/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_H4m8RZWuvO",
        "outputId": "8536f718-445e-4afc-d982-0d7ddac3101c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/models')\n",
        "import argparse\n",
        "import json,hashlib\n",
        "from transformers import AutoConfig, AutoModelForQuestionAnswering, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DefaultDataCollator, DataCollatorWithPadding\n",
        "import transformers\n",
        "from torch import nn\n",
        "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training, PeftModel\n",
        "from datasets import IterableDataset #, DataLoader\n",
        "import datasets\n",
        "import numpy as np\n",
        "import get_data\n",
        "import torch\n",
        "# from torch.utils.data import DataLoader\n",
        "import importlib\n",
        "import pathlib\n",
        "import re\n",
        "from typing import List\n",
        "try:\n",
        "    import requests\n",
        "except:\n",
        "    pass\n",
        "\n",
        "importlib.reload(get_data)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "def to_gpu_tensor(batch, device=device):\n",
        "    return {key: torch.tensor(value).to(device) for key, value in batch.items()}\n",
        "\n",
        "parser = argparse.ArgumentParser(\n",
        "    description=\"Example script\",\n",
        "    formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        ")\n",
        "# parser.add_argument('--batch_size', type=int, default=1, help=\"batch_size\")\n",
        "# parser.add_argument('--question_filename', type=str, default=\"questions_example.json\", help=\"question_filename\")\n",
        "# parser.add_argument('--write_path', type=str, default=\"/content/drive/MyDrive/models/predictions\", help=\"write_path\")\n",
        "# parser.add_argument('--df_start_id', type=int, default=0, help=\"df_start_id\")\n",
        "# parser.add_argument('--df_end_id', type=int, default=None, help=\"df_start_id\")\n",
        "# parser.add_argument('--model_dir_name', type=str, default=\"815aace0d964e9dad6ee4899b9ce83efea759b914d0a352c4e0cce1ad052d481\", help=\"model_dir_name\")\n",
        "# parser.add_argument('--checkpoint_num', type=str, default=\"120\", help=\"checkpoint_num\")\n",
        "# parser.add_argument('--ground_truths_filename', type=str, default=\"ALL.json\", help=\"checkpoint_num\")\n",
        "# parser.add_argument('--scan_ans_windowsize', type=int, default=20, help=\"scan_ans_windowsize\")\n",
        "# parser.add_argument('--requrl', type=str, default=None, help=\"invoke request\")\n",
        "\n",
        "# args = parser.parse_args()\n",
        "# parserargs_dict = vars(args)\n",
        "parserargs_dict = {\n",
        "    'batch_size':1,\n",
        "    'question_filename':'questions_preliminary.json',\n",
        "    'write_path':'/content/drive/MyDrive/models/predictions',\n",
        "    'df_start_id':500,\n",
        "    'model_dir_name':'815aace0d964e9dad6ee4899b9ce83efea759b914d0a352c4e0cce1ad052d481',\n",
        "    'checkpoint_num':600,\n",
        "    'df_end_id':None,\n",
        "    'ground_truths_filename':\"ALL120.json\",\n",
        "    'scan_ans_windowsize':20,\n",
        "    'requrl':None,\n",
        "}\n",
        "\n",
        "\n",
        "print(f\"parserargs_dict is {parserargs_dict}\")\n",
        "writepath = pathlib.Path(parserargs_dict['write_path'])\n",
        "model_base_dir = get_data.current_folder\n",
        "for target_create_path in [writepath, model_base_dir]:\n",
        "    target_create_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_base_dir = model_base_dir/parserargs_dict['model_dir_name']\n",
        "checkpoint_dir = model_base_dir/f\"checkpoint-{parserargs_dict['checkpoint_num']}\"\n",
        "argp_dict = (model_base_dir/\"argp_dict.json\").read_text()\n",
        "argp_dict = json.loads(argp_dict)\n",
        "# %%\n",
        "argp_dict['bnb_4bit_compute_dtype'] = torch.bfloat16 if argp_dict['bnb_4bit_compute_dtype']=='bf16' else torch.float32\n",
        "if argp_dict['load_in_8bit'] in [0,None,False] and argp_dict['load_in_4bit'] in [0,None,False]:\n",
        "    is_quantized = False\n",
        "    quantization_config = None\n",
        "else:\n",
        "    is_quantized = True\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=argp_dict['load_in_8bit'],\n",
        "        load_in_4bit=argp_dict['load_in_4bit'],\n",
        "        llm_int8_has_fp16_weight=argp_dict['llm_int8_has_fp16_weight'],\n",
        "        bnb_4bit_use_double_quant=argp_dict['bnb_4bit_use_double_quant'],\n",
        "        bnb_4bit_compute_dtype=argp_dict['bnb_4bit_compute_dtype'],\n",
        "        bnb_4bit_quant_type=argp_dict['bnb_4bit_quant_type'],\n",
        "    )\n",
        "\n",
        "if argp_dict['peft_r'] in [0,None,False] or argp_dict['peft_lora_alpha'] in [0,None,False]:\n",
        "    is_peft = False\n",
        "    peft_config = None\n",
        "else:\n",
        "    is_peft = True\n",
        "    peft_config = LoraConfig(\n",
        "        task_type=TaskType.QUESTION_ANS,\n",
        "        inference_mode=False,\n",
        "        r=argp_dict['peft_r'],\n",
        "        lora_alpha=argp_dict['peft_lora_alpha'],\n",
        "        lora_dropout=0.1\n",
        "    )\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "    checkpoint_dir,\n",
        "    token=get_data.access_token,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=quantization_config,\n",
        "    attn_implementation=argp_dict['attn_implementation'],\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "if is_quantized:\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "if is_peft:\n",
        "    model = PeftModel.from_pretrained(model, checkpoint_dir, is_trainable=False)\n",
        "print(\"model load complete\")\n",
        "# %%\n",
        "drop_cols_1 = ['start_positions', 'end_positions']\n",
        "drop_cols_2 = ['qid', 'question', 'category', 'context']\n",
        "\n",
        "# %%\n",
        "\n",
        "prediction_batch_size = parserargs_dict['batch_size']\n",
        "get_data_instance = get_data.GetdataClass(model_name=argp_dict['model_name'])\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiexO28nh5uA",
        "outputId": "4490aed2-d842-4647-f337-31345044f095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parserargs_dict is {'batch_size': 1, 'question_filename': 'questions_preliminary.json', 'write_path': '/content/drive/MyDrive/models/predictions', 'df_start_id': 500, 'model_dir_name': '815aace0d964e9dad6ee4899b9ce83efea759b914d0a352c4e0cce1ad052d481', 'checkpoint_num': 480, 'df_end_id': None, 'ground_truths_filename': 'ALL120.json', 'scan_ans_windowsize': 20, 'requrl': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model load complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "competition_dataframe = get_data_instance.get_competition_dataset_faq(join_truths=False, questions_filename=parserargs_dict['question_filename'])\n",
        "# parserargs_dict['df_start_id'] = 110\n",
        "competition_dataframe = competition_dataframe.sort_values(by=['qid'], ascending=False)#.iloc[0:parserargs_dict['df_start_id']].sort_values(by=['qid'], ascending=False)\n",
        "testset = datasets.Dataset.from_pandas(competition_dataframe.iloc[parserargs_dict['df_start_id']:parserargs_dict['df_end_id']])\n",
        "\n",
        "iterds_validation = testset \\\n",
        "    .map(get_data_instance.markup_article_id, batched=False, with_indices=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0557cdd915f842a895cd55480e828372",
            "77d35bd47c1c46d29183dbc8dfdc2698",
            "e0d879acc5484a5d8c8294c3a146341d",
            "59169a5324e6466387734854d7b99666",
            "2c570e7f9b3a44bf8e439f84c70afc13",
            "a0deadc2bf204b82a77fbff52a39c571",
            "6186545c6bf740e185bdd919d74604c3",
            "519c5de033e04b7cb73bad43edbfac9b",
            "755bf30c868349448a23018d5c99200d",
            "2b7590a66b7242a189afc21dff8ab090",
            "13c592573bec4fdba3e774f3b8fde603"
          ]
        },
        "id": "BnvDYlrRqIPG",
        "outputId": "6bf4a4f9-f0ff-481e-9105-3289f34229d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/790 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0557cdd915f842a895cd55480e828372"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=get_data_instance.tokenizer)\n",
        "# dataloader = DataLoader(\n",
        "#     iterds_validation.remove_columns(column_names=drop_cols_2),\n",
        "#     batch_size=prediction_batch_size,\n",
        "#     collate_fn=data_collator\n",
        "# )\n",
        "prediction_batch_size = 1\n",
        "iterds_validation_batched = iterds_validation.batch(prediction_batch_size)\n",
        "\n",
        "match_doc_id_pattern = re.compile(r'(?:insurance|finance|faq)_\\d+')\n",
        "def slice_list_by_list_of_int(srclist:List, indices:List)->List:\n",
        "    returnlist = []\n",
        "    for id in indices:\n",
        "        returnlist.append(srclist[id])\n",
        "    return returnlist\n",
        "def model_output_arrange(model_outputs, batchdata=None, scan_ans_windowsize_limit:int=25)->List:\n",
        "    global match_doc_id_pattern\n",
        "    func_batch_predictions_for_competition = []\n",
        "    max_seq_length = model_outputs.start_logits.shape[1]\n",
        "    print(f\"model_outputs.start_logits shape is {model_outputs.start_logits.shape}\")\n",
        "    pred_probs = tuple(torch.nn.functional.softmax(model_output, dim=-1) for model_output in [model_outputs.start_logits, model_outputs.end_logits])\n",
        "    pred_probs_argmax = tuple(prob.argmax(dim=-1) for prob in pred_probs)\n",
        "    reverse_start_end_indices = pred_probs_argmax[0]>pred_probs_argmax[1]\n",
        "    pred_pos = torch.stack(pred_probs_argmax, dim=-1)\n",
        "    pred_pos[:,0][reverse_start_end_indices] = pred_probs_argmax[1][reverse_start_end_indices]\n",
        "    pred_pos[:,1][reverse_start_end_indices] = pred_probs_argmax[0][reverse_start_end_indices]\n",
        "    for row_doc_i in range(pred_pos.shape[0]):\n",
        "        srctokens = batchdata['input_ids'][row_doc_i]\n",
        "        start = pred_pos[row_doc_i,0]\n",
        "        end = pred_pos[row_doc_i,1]\n",
        "        incre_num_by = 0\n",
        "        while True:\n",
        "            span = torch.arange(start, end+1)\n",
        "            span = slice_list_by_list_of_int(srctokens, span.tolist())\n",
        "            span = get_data_instance.tokenizer.decode(span)\n",
        "            matches = match_doc_id_pattern.findall(span)\n",
        "            if matches or (start==0 and end>=max_seq_length) or incre_num_by>scan_ans_windowsize_limit:\n",
        "                break\n",
        "            else:\n",
        "                incre_num_by += 5\n",
        "                start = 0 if (start - incre_num_by)<=0 else (start-incre_num_by)\n",
        "                end = 0 if (end + incre_num_by)<=0 else (end+incre_num_by)\n",
        "        if not matches:\n",
        "            pass\n",
        "        else:\n",
        "            pred_catg, pred_article_num = matches[0].split(\"_\")\n",
        "            func_batch_predictions_for_competition.append({\n",
        "                \"qid\": batchdata['qid'][row_doc_i],\n",
        "                \"category\": pred_catg,\n",
        "                \"retrieve\": pred_article_num,\n",
        "                \"incre_num_by\": incre_num_by\n",
        "            })\n",
        "            print(f\"{batchdata['qid'][row_doc_i]} done!\")\n",
        "    # print(f\"batch_predictions_for_competition {func_batch_predictions_for_competition}\")\n",
        "    return func_batch_predictions_for_competition\n",
        "\n",
        "print(\"dataloader complete\")\n",
        "overallpredictions = []\n",
        "with torch.no_grad():\n",
        "    print(f\"no_grad\")\n",
        "    for iter_i, orig in enumerate(iterds_validation_batched):\n",
        "        print(f\"iter_i {iter_i}\")\n",
        "        # print(f\"orig is {orig}\")\n",
        "        inputs = {key: val for key, val in orig.items() if key in ['input_ids', 'attention_mask']}\n",
        "        batch = data_collator(inputs)\n",
        "        batch = {key:value.to(device) for key,value in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        towrite_dicts = model_output_arrange(outputs, orig, parserargs_dict['scan_ans_windowsize'])\n",
        "        overallpredictions.extend(towrite_dicts)\n",
        "        target_file_name_suffix = f\"qid{orig['qid'][0]}_iter{iter_i}_checkpoint{parserargs_dict['checkpoint_num']}.json\"\n",
        "        target_file_name = writepath/target_file_name_suffix\n",
        "        with target_file_name.open('w', encoding='utf-8') as json_file:\n",
        "            json.dump(towrite_dicts, json_file, ensure_ascii=False, indent=4)\n",
        "        if parserargs_dict['requrl'] is not None:\n",
        "            try:\n",
        "                reqdict = {'filename':target_file_name_suffix,'towrite_dicts':towrite_dicts}\n",
        "                requests.post(parserargs_dict['requrl'], json=reqdict)\n",
        "            except Exception as e:\n",
        "                print(f\"sending req failed for {e}\")\n",
        "\n",
        "\n",
        "target_file_name = writepath/parserargs_dict['ground_truths_filename']\n",
        "overallpredictions = {\"ground_truths\":overallpredictions}\n",
        "with target_file_name.open('w', encoding='utf-8') as json_file:\n",
        "    json.dump(overallpredictions, json_file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "OmjvUC5uo42E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cf7421d-cfd4-4fca-dabe-7d17fcaa9720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "366 done!\n",
            "iter_i 256\n",
            "model_outputs.start_logits shape is torch.Size([1, 36833])\n",
            "367 done!\n",
            "iter_i 257\n",
            "model_outputs.start_logits shape is torch.Size([1, 7713])\n",
            "368 done!\n",
            "iter_i 258\n",
            "model_outputs.start_logits shape is torch.Size([1, 16337])\n",
            "369 done!\n",
            "iter_i 259\n",
            "model_outputs.start_logits shape is torch.Size([1, 16171])\n",
            "370 done!\n",
            "iter_i 260\n",
            "model_outputs.start_logits shape is torch.Size([1, 38114])\n",
            "371 done!\n",
            "iter_i 261\n",
            "model_outputs.start_logits shape is torch.Size([1, 33810])\n",
            "372 done!\n",
            "iter_i 262\n",
            "model_outputs.start_logits shape is torch.Size([1, 13483])\n",
            "373 done!\n",
            "iter_i 263\n",
            "model_outputs.start_logits shape is torch.Size([1, 7022])\n",
            "374 done!\n",
            "iter_i 264\n",
            "model_outputs.start_logits shape is torch.Size([1, 40390])\n",
            "375 done!\n",
            "iter_i 265\n",
            "model_outputs.start_logits shape is torch.Size([1, 8824])\n",
            "376 done!\n",
            "iter_i 266\n",
            "model_outputs.start_logits shape is torch.Size([1, 12612])\n",
            "377 done!\n",
            "iter_i 267\n",
            "model_outputs.start_logits shape is torch.Size([1, 35598])\n",
            "378 done!\n",
            "iter_i 268\n",
            "model_outputs.start_logits shape is torch.Size([1, 37164])\n",
            "379 done!\n",
            "iter_i 269\n",
            "model_outputs.start_logits shape is torch.Size([1, 30733])\n",
            "380 done!\n",
            "iter_i 270\n",
            "model_outputs.start_logits shape is torch.Size([1, 14926])\n",
            "381 done!\n",
            "iter_i 271\n",
            "model_outputs.start_logits shape is torch.Size([1, 41002])\n",
            "382 done!\n",
            "iter_i 272\n",
            "model_outputs.start_logits shape is torch.Size([1, 9956])\n",
            "383 done!\n",
            "iter_i 273\n",
            "model_outputs.start_logits shape is torch.Size([1, 11066])\n",
            "384 done!\n",
            "iter_i 274\n",
            "model_outputs.start_logits shape is torch.Size([1, 28735])\n",
            "385 done!\n",
            "iter_i 275\n",
            "model_outputs.start_logits shape is torch.Size([1, 17548])\n",
            "386 done!\n",
            "iter_i 276\n",
            "model_outputs.start_logits shape is torch.Size([1, 19626])\n",
            "387 done!\n",
            "iter_i 277\n",
            "model_outputs.start_logits shape is torch.Size([1, 18909])\n",
            "388 done!\n",
            "iter_i 278\n",
            "model_outputs.start_logits shape is torch.Size([1, 28822])\n",
            "389 done!\n",
            "iter_i 279\n",
            "model_outputs.start_logits shape is torch.Size([1, 25050])\n",
            "390 done!\n",
            "iter_i 280\n",
            "model_outputs.start_logits shape is torch.Size([1, 12908])\n",
            "391 done!\n",
            "iter_i 281\n",
            "model_outputs.start_logits shape is torch.Size([1, 8755])\n",
            "392 done!\n",
            "iter_i 282\n",
            "model_outputs.start_logits shape is torch.Size([1, 10657])\n",
            "393 done!\n",
            "iter_i 283\n",
            "model_outputs.start_logits shape is torch.Size([1, 14794])\n",
            "394 done!\n",
            "iter_i 284\n",
            "model_outputs.start_logits shape is torch.Size([1, 16924])\n",
            "395 done!\n",
            "iter_i 285\n",
            "model_outputs.start_logits shape is torch.Size([1, 22390])\n",
            "396 done!\n",
            "iter_i 286\n",
            "model_outputs.start_logits shape is torch.Size([1, 19814])\n",
            "397 done!\n",
            "iter_i 287\n",
            "model_outputs.start_logits shape is torch.Size([1, 44118])\n",
            "398 done!\n",
            "iter_i 288\n",
            "model_outputs.start_logits shape is torch.Size([1, 22529])\n",
            "399 done!\n",
            "iter_i 289\n",
            "model_outputs.start_logits shape is torch.Size([1, 11069])\n",
            "400 done!\n",
            "iter_i 290\n",
            "model_outputs.start_logits shape is torch.Size([1, 15396])\n",
            "401 done!\n",
            "iter_i 291\n",
            "model_outputs.start_logits shape is torch.Size([1, 21000])\n",
            "402 done!\n",
            "iter_i 292\n",
            "model_outputs.start_logits shape is torch.Size([1, 9580])\n",
            "403 done!\n",
            "iter_i 293\n",
            "model_outputs.start_logits shape is torch.Size([1, 18905])\n",
            "404 done!\n",
            "iter_i 294\n",
            "model_outputs.start_logits shape is torch.Size([1, 23760])\n",
            "405 done!\n",
            "iter_i 295\n",
            "model_outputs.start_logits shape is torch.Size([1, 27840])\n",
            "406 done!\n",
            "iter_i 296\n",
            "model_outputs.start_logits shape is torch.Size([1, 8398])\n",
            "407 done!\n",
            "iter_i 297\n",
            "model_outputs.start_logits shape is torch.Size([1, 13059])\n",
            "408 done!\n",
            "iter_i 298\n",
            "model_outputs.start_logits shape is torch.Size([1, 33462])\n",
            "409 done!\n",
            "iter_i 299\n",
            "model_outputs.start_logits shape is torch.Size([1, 6837])\n",
            "410 done!\n",
            "iter_i 300\n",
            "model_outputs.start_logits shape is torch.Size([1, 30520])\n",
            "411 done!\n",
            "iter_i 301\n",
            "model_outputs.start_logits shape is torch.Size([1, 15275])\n",
            "412 done!\n",
            "iter_i 302\n",
            "model_outputs.start_logits shape is torch.Size([1, 17548])\n",
            "413 done!\n",
            "iter_i 303\n",
            "model_outputs.start_logits shape is torch.Size([1, 7649])\n",
            "414 done!\n",
            "iter_i 304\n",
            "model_outputs.start_logits shape is torch.Size([1, 7057])\n",
            "415 done!\n",
            "iter_i 305\n",
            "model_outputs.start_logits shape is torch.Size([1, 15430])\n",
            "416 done!\n",
            "iter_i 306\n",
            "model_outputs.start_logits shape is torch.Size([1, 10622])\n",
            "417 done!\n",
            "iter_i 307\n",
            "model_outputs.start_logits shape is torch.Size([1, 24332])\n",
            "418 done!\n",
            "iter_i 308\n",
            "model_outputs.start_logits shape is torch.Size([1, 17827])\n",
            "419 done!\n",
            "iter_i 309\n",
            "model_outputs.start_logits shape is torch.Size([1, 14207])\n",
            "420 done!\n",
            "iter_i 310\n",
            "model_outputs.start_logits shape is torch.Size([1, 39137])\n",
            "421 done!\n",
            "iter_i 311\n",
            "model_outputs.start_logits shape is torch.Size([1, 23135])\n",
            "422 done!\n",
            "iter_i 312\n",
            "model_outputs.start_logits shape is torch.Size([1, 12187])\n",
            "423 done!\n",
            "iter_i 313\n",
            "model_outputs.start_logits shape is torch.Size([1, 7393])\n",
            "424 done!\n",
            "iter_i 314\n",
            "model_outputs.start_logits shape is torch.Size([1, 16856])\n",
            "425 done!\n",
            "iter_i 315\n",
            "model_outputs.start_logits shape is torch.Size([1, 22882])\n",
            "426 done!\n",
            "iter_i 316\n",
            "model_outputs.start_logits shape is torch.Size([1, 18248])\n",
            "427 done!\n",
            "iter_i 317\n",
            "model_outputs.start_logits shape is torch.Size([1, 18431])\n",
            "428 done!\n",
            "iter_i 318\n",
            "model_outputs.start_logits shape is torch.Size([1, 23896])\n",
            "429 done!\n",
            "iter_i 319\n",
            "model_outputs.start_logits shape is torch.Size([1, 21058])\n",
            "430 done!\n",
            "iter_i 320\n",
            "model_outputs.start_logits shape is torch.Size([1, 15251])\n",
            "431 done!\n",
            "iter_i 321\n",
            "model_outputs.start_logits shape is torch.Size([1, 16923])\n",
            "432 done!\n",
            "iter_i 322\n",
            "model_outputs.start_logits shape is torch.Size([1, 12552])\n",
            "433 done!\n",
            "iter_i 323\n",
            "model_outputs.start_logits shape is torch.Size([1, 32064])\n",
            "434 done!\n",
            "iter_i 324\n",
            "model_outputs.start_logits shape is torch.Size([1, 38068])\n",
            "435 done!\n",
            "iter_i 325\n",
            "model_outputs.start_logits shape is torch.Size([1, 37661])\n",
            "436 done!\n",
            "iter_i 326\n",
            "model_outputs.start_logits shape is torch.Size([1, 14399])\n",
            "437 done!\n",
            "iter_i 327\n",
            "model_outputs.start_logits shape is torch.Size([1, 12238])\n",
            "438 done!\n",
            "iter_i 328\n",
            "model_outputs.start_logits shape is torch.Size([1, 31395])\n",
            "439 done!\n",
            "iter_i 329\n",
            "model_outputs.start_logits shape is torch.Size([1, 12595])\n",
            "440 done!\n",
            "iter_i 330\n",
            "model_outputs.start_logits shape is torch.Size([1, 6595])\n",
            "441 done!\n",
            "iter_i 331\n",
            "model_outputs.start_logits shape is torch.Size([1, 18433])\n",
            "442 done!\n",
            "iter_i 332\n",
            "model_outputs.start_logits shape is torch.Size([1, 25451])\n",
            "443 done!\n",
            "iter_i 333\n",
            "model_outputs.start_logits shape is torch.Size([1, 21691])\n",
            "444 done!\n",
            "iter_i 334\n",
            "model_outputs.start_logits shape is torch.Size([1, 15937])\n",
            "445 done!\n",
            "iter_i 335\n",
            "model_outputs.start_logits shape is torch.Size([1, 31213])\n",
            "446 done!\n",
            "iter_i 336\n",
            "model_outputs.start_logits shape is torch.Size([1, 8430])\n",
            "447 done!\n",
            "iter_i 337\n",
            "model_outputs.start_logits shape is torch.Size([1, 23369])\n",
            "448 done!\n",
            "iter_i 338\n",
            "model_outputs.start_logits shape is torch.Size([1, 30639])\n",
            "449 done!\n",
            "iter_i 339\n",
            "model_outputs.start_logits shape is torch.Size([1, 37170])\n",
            "450 done!\n",
            "iter_i 340\n",
            "model_outputs.start_logits shape is torch.Size([1, 13476])\n",
            "451 done!\n",
            "iter_i 341\n",
            "model_outputs.start_logits shape is torch.Size([1, 12935])\n",
            "452 done!\n",
            "iter_i 342\n",
            "model_outputs.start_logits shape is torch.Size([1, 20473])\n",
            "453 done!\n",
            "iter_i 343\n",
            "model_outputs.start_logits shape is torch.Size([1, 41258])\n",
            "454 done!\n",
            "iter_i 344\n",
            "model_outputs.start_logits shape is torch.Size([1, 14384])\n",
            "455 done!\n",
            "iter_i 345\n",
            "model_outputs.start_logits shape is torch.Size([1, 13509])\n",
            "456 done!\n",
            "iter_i 346\n",
            "model_outputs.start_logits shape is torch.Size([1, 23898])\n",
            "457 done!\n",
            "iter_i 347\n",
            "model_outputs.start_logits shape is torch.Size([1, 35177])\n",
            "458 done!\n",
            "iter_i 348\n",
            "model_outputs.start_logits shape is torch.Size([1, 22519])\n",
            "459 done!\n",
            "iter_i 349\n",
            "model_outputs.start_logits shape is torch.Size([1, 15410])\n",
            "460 done!\n",
            "iter_i 350\n",
            "model_outputs.start_logits shape is torch.Size([1, 12404])\n",
            "461 done!\n",
            "iter_i 351\n",
            "model_outputs.start_logits shape is torch.Size([1, 6879])\n",
            "462 done!\n",
            "iter_i 352\n",
            "model_outputs.start_logits shape is torch.Size([1, 9124])\n",
            "463 done!\n",
            "iter_i 353\n",
            "model_outputs.start_logits shape is torch.Size([1, 19625])\n",
            "464 done!\n",
            "iter_i 354\n",
            "model_outputs.start_logits shape is torch.Size([1, 25105])\n",
            "465 done!\n",
            "iter_i 355\n",
            "model_outputs.start_logits shape is torch.Size([1, 24551])\n",
            "466 done!\n",
            "iter_i 356\n",
            "model_outputs.start_logits shape is torch.Size([1, 13474])\n",
            "467 done!\n",
            "iter_i 357\n",
            "model_outputs.start_logits shape is torch.Size([1, 27324])\n",
            "468 done!\n",
            "iter_i 358\n",
            "model_outputs.start_logits shape is torch.Size([1, 20271])\n",
            "469 done!\n",
            "iter_i 359\n",
            "model_outputs.start_logits shape is torch.Size([1, 23725])\n",
            "470 done!\n",
            "iter_i 360\n",
            "model_outputs.start_logits shape is torch.Size([1, 30354])\n",
            "471 done!\n",
            "iter_i 361\n",
            "model_outputs.start_logits shape is torch.Size([1, 16838])\n",
            "472 done!\n",
            "iter_i 362\n",
            "model_outputs.start_logits shape is torch.Size([1, 22884])\n",
            "473 done!\n",
            "iter_i 363\n",
            "model_outputs.start_logits shape is torch.Size([1, 6847])\n",
            "474 done!\n",
            "iter_i 364\n",
            "model_outputs.start_logits shape is torch.Size([1, 27970])\n",
            "475 done!\n",
            "iter_i 365\n",
            "model_outputs.start_logits shape is torch.Size([1, 33306])\n",
            "476 done!\n",
            "iter_i 366\n",
            "model_outputs.start_logits shape is torch.Size([1, 11873])\n",
            "477 done!\n",
            "iter_i 367\n",
            "model_outputs.start_logits shape is torch.Size([1, 27694])\n",
            "478 done!\n",
            "iter_i 368\n",
            "model_outputs.start_logits shape is torch.Size([1, 6333])\n",
            "479 done!\n",
            "iter_i 369\n",
            "model_outputs.start_logits shape is torch.Size([1, 36481])\n",
            "480 done!\n",
            "iter_i 370\n",
            "model_outputs.start_logits shape is torch.Size([1, 7966])\n",
            "481 done!\n",
            "iter_i 371\n",
            "model_outputs.start_logits shape is torch.Size([1, 25310])\n",
            "482 done!\n",
            "iter_i 372\n",
            "model_outputs.start_logits shape is torch.Size([1, 19216])\n",
            "483 done!\n",
            "iter_i 373\n",
            "model_outputs.start_logits shape is torch.Size([1, 14214])\n",
            "484 done!\n",
            "iter_i 374\n",
            "model_outputs.start_logits shape is torch.Size([1, 16720])\n",
            "485 done!\n",
            "iter_i 375\n",
            "model_outputs.start_logits shape is torch.Size([1, 7589])\n",
            "486 done!\n",
            "iter_i 376\n",
            "model_outputs.start_logits shape is torch.Size([1, 20467])\n",
            "487 done!\n",
            "iter_i 377\n",
            "model_outputs.start_logits shape is torch.Size([1, 40396])\n",
            "488 done!\n",
            "iter_i 378\n",
            "model_outputs.start_logits shape is torch.Size([1, 7665])\n",
            "489 done!\n",
            "iter_i 379\n",
            "model_outputs.start_logits shape is torch.Size([1, 14142])\n",
            "490 done!\n",
            "iter_i 380\n",
            "model_outputs.start_logits shape is torch.Size([1, 9542])\n",
            "491 done!\n",
            "iter_i 381\n",
            "model_outputs.start_logits shape is torch.Size([1, 15124])\n",
            "492 done!\n",
            "iter_i 382\n",
            "model_outputs.start_logits shape is torch.Size([1, 7519])\n",
            "493 done!\n",
            "iter_i 383\n",
            "model_outputs.start_logits shape is torch.Size([1, 8707])\n",
            "494 done!\n",
            "iter_i 384\n",
            "model_outputs.start_logits shape is torch.Size([1, 25151])\n",
            "495 done!\n",
            "iter_i 385\n",
            "model_outputs.start_logits shape is torch.Size([1, 36482])\n",
            "496 done!\n",
            "iter_i 386\n",
            "model_outputs.start_logits shape is torch.Size([1, 7308])\n",
            "497 done!\n",
            "iter_i 387\n",
            "model_outputs.start_logits shape is torch.Size([1, 14014])\n",
            "498 done!\n",
            "iter_i 388\n",
            "model_outputs.start_logits shape is torch.Size([1, 20963])\n",
            "499 done!\n",
            "iter_i 389\n",
            "model_outputs.start_logits shape is torch.Size([1, 14932])\n",
            "500 done!\n",
            "iter_i 390\n",
            "model_outputs.start_logits shape is torch.Size([1, 17949])\n",
            "501 done!\n",
            "iter_i 391\n",
            "model_outputs.start_logits shape is torch.Size([1, 11049])\n",
            "502 done!\n",
            "iter_i 392\n",
            "model_outputs.start_logits shape is torch.Size([1, 28337])\n",
            "503 done!\n",
            "iter_i 393\n",
            "model_outputs.start_logits shape is torch.Size([1, 38048])\n",
            "504 done!\n",
            "iter_i 394\n",
            "model_outputs.start_logits shape is torch.Size([1, 22952])\n",
            "505 done!\n",
            "iter_i 395\n",
            "model_outputs.start_logits shape is torch.Size([1, 21847])\n",
            "506 done!\n",
            "iter_i 396\n",
            "model_outputs.start_logits shape is torch.Size([1, 6897])\n",
            "507 done!\n",
            "iter_i 397\n",
            "model_outputs.start_logits shape is torch.Size([1, 10848])\n",
            "508 done!\n",
            "iter_i 398\n",
            "model_outputs.start_logits shape is torch.Size([1, 34647])\n",
            "509 done!\n",
            "iter_i 399\n",
            "model_outputs.start_logits shape is torch.Size([1, 14093])\n",
            "510 done!\n",
            "iter_i 400\n",
            "model_outputs.start_logits shape is torch.Size([1, 11877])\n",
            "511 done!\n",
            "iter_i 401\n",
            "model_outputs.start_logits shape is torch.Size([1, 16207])\n",
            "512 done!\n",
            "iter_i 402\n",
            "model_outputs.start_logits shape is torch.Size([1, 13237])\n",
            "513 done!\n",
            "iter_i 403\n",
            "model_outputs.start_logits shape is torch.Size([1, 30082])\n",
            "514 done!\n",
            "iter_i 404\n",
            "model_outputs.start_logits shape is torch.Size([1, 14060])\n",
            "515 done!\n",
            "iter_i 405\n",
            "model_outputs.start_logits shape is torch.Size([1, 33045])\n",
            "516 done!\n",
            "iter_i 406\n",
            "model_outputs.start_logits shape is torch.Size([1, 13867])\n",
            "517 done!\n",
            "iter_i 407\n",
            "model_outputs.start_logits shape is torch.Size([1, 12234])\n",
            "518 done!\n",
            "iter_i 408\n",
            "model_outputs.start_logits shape is torch.Size([1, 22884])\n",
            "519 done!\n",
            "iter_i 409\n",
            "model_outputs.start_logits shape is torch.Size([1, 17292])\n",
            "520 done!\n",
            "iter_i 410\n",
            "model_outputs.start_logits shape is torch.Size([1, 20036])\n",
            "521 done!\n",
            "iter_i 411\n",
            "model_outputs.start_logits shape is torch.Size([1, 12234])\n",
            "522 done!\n",
            "iter_i 412\n",
            "model_outputs.start_logits shape is torch.Size([1, 33805])\n",
            "523 done!\n",
            "iter_i 413\n",
            "model_outputs.start_logits shape is torch.Size([1, 30358])\n",
            "524 done!\n",
            "iter_i 414\n",
            "model_outputs.start_logits shape is torch.Size([1, 18264])\n",
            "525 done!\n",
            "iter_i 415\n",
            "model_outputs.start_logits shape is torch.Size([1, 17617])\n",
            "526 done!\n",
            "iter_i 416\n",
            "model_outputs.start_logits shape is torch.Size([1, 6108])\n",
            "527 done!\n",
            "iter_i 417\n",
            "model_outputs.start_logits shape is torch.Size([1, 31692])\n",
            "528 done!\n",
            "iter_i 418\n",
            "model_outputs.start_logits shape is torch.Size([1, 6749])\n",
            "529 done!\n",
            "iter_i 419\n",
            "model_outputs.start_logits shape is torch.Size([1, 14459])\n",
            "530 done!\n",
            "iter_i 420\n",
            "model_outputs.start_logits shape is torch.Size([1, 21428])\n",
            "531 done!\n",
            "iter_i 421\n",
            "model_outputs.start_logits shape is torch.Size([1, 14924])\n",
            "532 done!\n",
            "iter_i 422\n",
            "model_outputs.start_logits shape is torch.Size([1, 32933])\n",
            "533 done!\n",
            "iter_i 423\n",
            "model_outputs.start_logits shape is torch.Size([1, 18428])\n",
            "534 done!\n",
            "iter_i 424\n",
            "model_outputs.start_logits shape is torch.Size([1, 37172])\n",
            "535 done!\n",
            "iter_i 425\n",
            "model_outputs.start_logits shape is torch.Size([1, 30639])\n",
            "536 done!\n",
            "iter_i 426\n",
            "model_outputs.start_logits shape is torch.Size([1, 13358])\n",
            "537 done!\n",
            "iter_i 427\n",
            "model_outputs.start_logits shape is torch.Size([1, 14994])\n",
            "538 done!\n",
            "iter_i 428\n",
            "model_outputs.start_logits shape is torch.Size([1, 16732])\n",
            "539 done!\n",
            "iter_i 429\n",
            "model_outputs.start_logits shape is torch.Size([1, 19226])\n",
            "540 done!\n",
            "iter_i 430\n",
            "model_outputs.start_logits shape is torch.Size([1, 15113])\n",
            "541 done!\n",
            "iter_i 431\n",
            "model_outputs.start_logits shape is torch.Size([1, 23445])\n",
            "542 done!\n",
            "iter_i 432\n",
            "model_outputs.start_logits shape is torch.Size([1, 17558])\n",
            "543 done!\n",
            "iter_i 433\n",
            "model_outputs.start_logits shape is torch.Size([1, 10862])\n",
            "544 done!\n",
            "iter_i 434\n",
            "model_outputs.start_logits shape is torch.Size([1, 16837])\n",
            "545 done!\n",
            "iter_i 435\n",
            "model_outputs.start_logits shape is torch.Size([1, 21076])\n",
            "546 done!\n",
            "iter_i 436\n",
            "model_outputs.start_logits shape is torch.Size([1, 16153])\n",
            "547 done!\n",
            "iter_i 437\n",
            "model_outputs.start_logits shape is torch.Size([1, 12632])\n",
            "548 done!\n",
            "iter_i 438\n",
            "model_outputs.start_logits shape is torch.Size([1, 7958])\n",
            "549 done!\n",
            "iter_i 439\n",
            "model_outputs.start_logits shape is torch.Size([1, 18259])\n",
            "550 done!\n",
            "iter_i 440\n",
            "model_outputs.start_logits shape is torch.Size([1, 20799])\n",
            "551 done!\n",
            "iter_i 441\n",
            "model_outputs.start_logits shape is torch.Size([1, 25453])\n",
            "552 done!\n",
            "iter_i 442\n",
            "model_outputs.start_logits shape is torch.Size([1, 14847])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/models/inference_simpleqa.py --batch_size=2 --question_filename=questions_preliminary.json --write_path=/content/drive/MyDrive/models/predictions --df_start_id=500 --checkpoint_num=120 --ground_truths_filename=checkpoint120_all.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgODxBPMyjhr",
        "outputId": "ea370f58-0cec-47cf-c8b6-7cafeb655923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-09 06:03:43.174839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-09 06:03:43.211921: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-09 06:03:43.221789: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-09 06:03:43.245422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-09 06:03:44.954411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "parserargs_dict is {'batch_size': 2, 'question_filename': 'questions_preliminary.json', 'write_path': '/content/drive/MyDrive/models/predictions', 'df_start_id': 500, 'df_end_id': None, 'model_dir_name': '815aace0d964e9dad6ee4899b9ce83efea759b914d0a352c4e0cce1ad052d481', 'checkpoint_num': '120', 'ground_truths_filename': 'checkpoint120_all.json', 'scan_ans_windowsize': 20, 'requrl': None}\n",
            "Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "model load complete\n",
            "dataloader complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q97XDdK8Vx_o",
        "outputId": "db7b0d01-6762-4fb1-e2c6-5e4ca5aa1a3d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-05 22:04:17.670026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-05 22:04:17.689880: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-05 22:04:17.695906: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-05 22:04:17.710501: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-05 22:04:18.853075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "argp_dict: {'batch_size': 1, 'gradient_accumulation_steps': 4, 'torch_empty_cache_steps': 2, 'peft_r': 4, 'torch_compile': False}\n",
            "Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "trainable params: 272130 || all params: 494306692 || trainable%: 0.06\n",
            "Fold 0:\n",
            "  Train: 120 examples\n",
            "  Test: 30 examples\n",
            "Map: 100% 30/30 [00:03<00:00,  7.87 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "  0% 0/1200 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{'loss': 9.7444, 'grad_norm': 655.0924682617188, 'learning_rate': 1.811666666666667e-05, 'epoch': 0.1}\n",
            " 10% 120/1200 [09:48<20:04,  1.12s/it]\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:12,  2.23it/s]\u001b[A\n",
            " 10% 3/30 [00:01<00:09,  2.82it/s]\u001b[A\n",
            " 13% 4/30 [00:01<00:11,  2.35it/s]\u001b[A\n",
            " 17% 5/30 [00:02<00:10,  2.29it/s]\u001b[A\n",
            " 20% 6/30 [00:02<00:10,  2.20it/s]\u001b[A\n",
            " 23% 7/30 [00:02<00:09,  2.32it/s]\u001b[A\n",
            " 27% 8/30 [00:03<00:12,  1.82it/s]\u001b[A\n",
            " 30% 9/30 [00:04<00:11,  1.82it/s]\u001b[A\n",
            " 33% 10/30 [00:04<00:10,  1.97it/s]\u001b[A\n",
            " 37% 11/30 [00:05<00:10,  1.83it/s]\u001b[A\n",
            " 40% 12/30 [00:06<00:14,  1.22it/s]\u001b[A\n",
            " 43% 13/30 [00:08<00:18,  1.11s/it]\u001b[A\n",
            " 47% 14/30 [00:09<00:16,  1.03s/it]\u001b[A\n",
            " 50% 15/30 [00:10<00:13,  1.13it/s]\u001b[A\n",
            " 53% 16/30 [00:11<00:14,  1.03s/it]\u001b[A\n",
            " 57% 17/30 [00:13<00:17,  1.31s/it]\u001b[A\n",
            " 60% 18/30 [00:14<00:14,  1.22s/it]\u001b[A\n",
            " 63% 19/30 [00:18<00:22,  2.02s/it]\u001b[A\n",
            " 67% 20/30 [00:19<00:19,  1.92s/it]\u001b[A\n",
            " 70% 21/30 [00:20<00:12,  1.40s/it]\u001b[A\n",
            " 73% 22/30 [00:20<00:08,  1.02s/it]\u001b[A\n",
            " 77% 23/30 [00:20<00:05,  1.30it/s]\u001b[A\n",
            " 80% 24/30 [00:20<00:03,  1.71it/s]\u001b[A\n",
            " 83% 25/30 [00:20<00:02,  2.21it/s]\u001b[A\n",
            " 87% 26/30 [00:20<00:01,  2.71it/s]\u001b[A\n",
            " 90% 27/30 [00:21<00:00,  3.21it/s]\u001b[A\n",
            " 93% 28/30 [00:21<00:00,  3.70it/s]\u001b[A\n",
            " 97% 29/30 [00:21<00:00,  4.17it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 7.282291889190674, 'eval_precision': 0.0011417681369435958, 'eval_recall': 0.4, 'eval_ExactMatch': 0.0, 'eval_runtime': 22.504, 'eval_samples_per_second': 1.333, 'eval_steps_per_second': 1.333, 'epoch': 0.1}\n",
            " 10% 120/1200 [10:10<20:04,  1.12s/it]\n",
            "100% 30/30 [00:21<00:00,  4.61it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{'loss': 6.2324, 'grad_norm': 489.80535888671875, 'learning_rate': 1.6116666666666668e-05, 'epoch': 0.2}\n",
            " 20% 240/1200 [19:57<17:45,  1.11s/it]\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:12,  2.23it/s]\u001b[A\n",
            " 10% 3/30 [00:01<00:09,  2.81it/s]\u001b[A\n",
            " 13% 4/30 [00:01<00:11,  2.36it/s]\u001b[A\n",
            " 17% 5/30 [00:02<00:10,  2.29it/s]\u001b[A\n",
            " 20% 6/30 [00:02<00:10,  2.20it/s]\u001b[A\n",
            " 23% 7/30 [00:02<00:09,  2.31it/s]\u001b[A\n",
            " 27% 8/30 [00:03<00:12,  1.81it/s]\u001b[A\n",
            " 30% 9/30 [00:04<00:11,  1.81it/s]\u001b[A\n",
            " 33% 10/30 [00:04<00:10,  1.95it/s]\u001b[A\n",
            " 37% 11/30 [00:05<00:10,  1.82it/s]\u001b[A\n",
            " 40% 12/30 [00:06<00:14,  1.21it/s]\u001b[A\n",
            " 43% 13/30 [00:08<00:18,  1.11s/it]\u001b[A\n",
            " 47% 14/30 [00:09<00:16,  1.03s/it]\u001b[A\n",
            " 50% 15/30 [00:10<00:13,  1.13it/s]\u001b[A\n",
            " 53% 16/30 [00:11<00:14,  1.03s/it]\u001b[A\n",
            " 57% 17/30 [00:13<00:17,  1.31s/it]\u001b[A\n",
            " 60% 18/30 [00:14<00:14,  1.22s/it]\u001b[A\n",
            " 63% 19/30 [00:18<00:22,  2.02s/it]\u001b[A\n",
            " 67% 20/30 [00:19<00:19,  1.92s/it]\u001b[A\n",
            " 70% 21/30 [00:20<00:12,  1.40s/it]\u001b[A\n",
            " 73% 22/30 [00:20<00:08,  1.02s/it]\u001b[A\n",
            " 77% 23/30 [00:20<00:05,  1.30it/s]\u001b[A\n",
            " 80% 24/30 [00:20<00:03,  1.71it/s]\u001b[A\n",
            " 83% 25/30 [00:20<00:02,  2.20it/s]\u001b[A\n",
            " 87% 26/30 [00:20<00:01,  2.71it/s]\u001b[A\n",
            " 90% 27/30 [00:21<00:00,  3.22it/s]\u001b[A\n",
            " 93% 28/30 [00:21<00:00,  3.69it/s]\u001b[A\n",
            " 97% 29/30 [00:21<00:00,  4.18it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 4.542545795440674, 'eval_precision': 0.0009098321838599244, 'eval_recall': 0.22777777777777777, 'eval_ExactMatch': 0.0, 'eval_runtime': 22.4844, 'eval_samples_per_second': 1.334, 'eval_steps_per_second': 1.334, 'epoch': 0.2}\n",
            " 20% 240/1200 [20:20<17:45,  1.11s/it]\n",
            "100% 30/30 [00:21<00:00,  4.63it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{'loss': 4.2536, 'grad_norm': 212.84225463867188, 'learning_rate': 1.4116666666666668e-05, 'epoch': 0.3}\n",
            " 30% 360/1200 [30:07<16:20,  1.17s/it]\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:12,  2.24it/s]\u001b[A\n",
            " 10% 3/30 [00:01<00:09,  2.82it/s]\u001b[A\n",
            " 13% 4/30 [00:01<00:10,  2.36it/s]\u001b[A\n",
            " 17% 5/30 [00:02<00:10,  2.30it/s]\u001b[A\n",
            " 20% 6/30 [00:02<00:10,  2.23it/s]\u001b[A\n",
            " 23% 7/30 [00:02<00:09,  2.35it/s]\u001b[A\n",
            " 27% 8/30 [00:03<00:12,  1.83it/s]\u001b[A\n",
            " 30% 9/30 [00:04<00:11,  1.82it/s]\u001b[A\n",
            " 33% 10/30 [00:04<00:10,  1.97it/s]\u001b[A\n",
            " 37% 11/30 [00:05<00:10,  1.84it/s]\u001b[A\n",
            " 40% 12/30 [00:06<00:14,  1.22it/s]\u001b[A\n",
            " 43% 13/30 [00:08<00:18,  1.10s/it]\u001b[A\n",
            " 47% 14/30 [00:09<00:16,  1.03s/it]\u001b[A\n",
            " 50% 15/30 [00:09<00:13,  1.13it/s]\u001b[A\n",
            " 53% 16/30 [00:11<00:14,  1.02s/it]\u001b[A\n",
            " 57% 17/30 [00:13<00:17,  1.31s/it]\u001b[A\n",
            " 60% 18/30 [00:14<00:14,  1.23s/it]\u001b[A\n",
            " 63% 19/30 [00:18<00:22,  2.03s/it]\u001b[A\n",
            " 67% 20/30 [00:19<00:19,  1.94s/it]\u001b[A\n",
            " 70% 21/30 [00:20<00:12,  1.40s/it]\u001b[A\n",
            " 73% 22/30 [00:20<00:08,  1.02s/it]\u001b[A\n",
            " 77% 23/30 [00:20<00:05,  1.30it/s]\u001b[A\n",
            " 80% 24/30 [00:20<00:03,  1.70it/s]\u001b[A\n",
            " 83% 25/30 [00:20<00:02,  2.21it/s]\u001b[A\n",
            " 87% 26/30 [00:20<00:01,  2.72it/s]\u001b[A\n",
            " 90% 27/30 [00:21<00:00,  3.23it/s]\u001b[A\n",
            " 93% 28/30 [00:21<00:00,  3.71it/s]\u001b[A\n",
            " 97% 29/30 [00:21<00:00,  4.20it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 3.5845296382904053, 'eval_precision': 0.001575092413817332, 'eval_recall': 0.33999999999999997, 'eval_ExactMatch': 0.0, 'eval_runtime': 22.4849, 'eval_samples_per_second': 1.334, 'eval_steps_per_second': 1.334, 'epoch': 0.3}\n",
            " 30% 360/1200 [30:29<16:20,  1.17s/it]\n",
            "100% 30/30 [00:21<00:00,  4.64it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{'loss': 3.578, 'grad_norm': 132.88990783691406, 'learning_rate': 1.2116666666666667e-05, 'epoch': 0.4}\n",
            " 40% 480/1200 [40:19<13:39,  1.14s/it]\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:12,  2.25it/s]\u001b[A\n",
            " 10% 3/30 [00:01<00:09,  2.84it/s]\u001b[A\n",
            " 13% 4/30 [00:01<00:10,  2.38it/s]\u001b[A\n",
            " 17% 5/30 [00:02<00:10,  2.31it/s]\u001b[A\n",
            " 20% 6/30 [00:02<00:10,  2.23it/s]\u001b[A\n",
            " 23% 7/30 [00:02<00:09,  2.35it/s]\u001b[A\n",
            " 27% 8/30 [00:03<00:11,  1.84it/s]\u001b[A\n",
            " 30% 9/30 [00:04<00:11,  1.83it/s]\u001b[A\n",
            " 33% 10/30 [00:04<00:10,  1.97it/s]\u001b[A\n",
            " 37% 11/30 [00:05<00:10,  1.84it/s]\u001b[A\n",
            " 40% 12/30 [00:06<00:14,  1.23it/s]\u001b[A\n",
            " 43% 13/30 [00:08<00:18,  1.10s/it]\u001b[A\n",
            " 47% 14/30 [00:09<00:16,  1.03s/it]\u001b[A\n",
            " 50% 15/30 [00:09<00:13,  1.13it/s]\u001b[A\n",
            " 53% 16/30 [00:11<00:14,  1.03s/it]\u001b[A\n",
            " 57% 17/30 [00:13<00:17,  1.32s/it]\u001b[A\n",
            " 60% 18/30 [00:14<00:14,  1.22s/it]\u001b[A\n",
            " 63% 19/30 [00:18<00:22,  2.02s/it]\u001b[A\n",
            " 67% 20/30 [00:19<00:19,  1.92s/it]\u001b[A\n",
            " 70% 21/30 [00:20<00:12,  1.40s/it]\u001b[A\n",
            " 73% 22/30 [00:20<00:08,  1.02s/it]\u001b[A\n",
            " 77% 23/30 [00:20<00:05,  1.30it/s]\u001b[A\n",
            " 80% 24/30 [00:20<00:03,  1.71it/s]\u001b[A\n",
            " 83% 25/30 [00:20<00:02,  2.22it/s]\u001b[A\n",
            " 87% 26/30 [00:20<00:01,  2.74it/s]\u001b[A\n",
            " 90% 27/30 [00:21<00:00,  3.24it/s]\u001b[A\n",
            " 93% 28/30 [00:21<00:00,  3.73it/s]\u001b[A\n",
            " 97% 29/30 [00:21<00:00,  4.22it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 3.271085500717163, 'eval_precision': 0.0015723169220113057, 'eval_recall': 0.32888888888888895, 'eval_ExactMatch': 0.0, 'eval_runtime': 22.3933, 'eval_samples_per_second': 1.34, 'eval_steps_per_second': 1.34, 'epoch': 0.4}\n",
            " 40% 480/1200 [40:41<13:39,  1.14s/it]\n",
            "100% 30/30 [00:21<00:00,  4.67it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{'loss': 3.2841, 'grad_norm': 102.02225494384766, 'learning_rate': 1.0116666666666667e-05, 'epoch': 0.5}\n",
            " 50% 600/1200 [50:27<11:21,  1.14s/it]\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:12,  2.23it/s]\u001b[A\n",
            " 10% 3/30 [00:01<00:09,  2.81it/s]\u001b[A\n",
            " 13% 4/30 [00:01<00:11,  2.35it/s]\u001b[A\n",
            " 17% 5/30 [00:02<00:10,  2.28it/s]\u001b[A\n",
            " 20% 6/30 [00:02<00:10,  2.22it/s]\u001b[A\n",
            " 23% 7/30 [00:02<00:09,  2.34it/s]\u001b[A\n",
            " 27% 8/30 [00:03<00:12,  1.83it/s]\u001b[A\n",
            " 30% 9/30 [00:04<00:11,  1.83it/s]\u001b[A\n",
            " 33% 10/30 [00:04<00:10,  1.97it/s]\u001b[A\n",
            " 37% 11/30 [00:05<00:10,  1.84it/s]\u001b[A\n",
            " 40% 12/30 [00:06<00:14,  1.23it/s]\u001b[A\n",
            " 43% 13/30 [00:08<00:18,  1.10s/it]\u001b[A\n",
            " 47% 14/30 [00:09<00:16,  1.02s/it]\u001b[A\n",
            " 50% 15/30 [00:09<00:13,  1.14it/s]\u001b[A\n",
            " 53% 16/30 [00:11<00:14,  1.02s/it]\u001b[A\n",
            " 57% 17/30 [00:13<00:17,  1.31s/it]\u001b[A\n",
            " 60% 18/30 [00:14<00:14,  1.22s/it]\u001b[A\n",
            " 63% 19/30 [00:18<00:22,  2.02s/it]\u001b[A\n",
            " 67% 20/30 [00:19<00:19,  1.93s/it]\u001b[A\n",
            " 70% 21/30 [00:20<00:12,  1.40s/it]\u001b[A\n",
            " 73% 22/30 [00:20<00:08,  1.02s/it]\u001b[A\n",
            " 77% 23/30 [00:20<00:05,  1.30it/s]\u001b[A\n",
            " 80% 24/30 [00:20<00:03,  1.71it/s]\u001b[A\n",
            " 83% 25/30 [00:20<00:02,  2.22it/s]\u001b[A\n",
            " 87% 26/30 [00:20<00:01,  2.73it/s]\u001b[A\n",
            " 90% 27/30 [00:21<00:00,  3.24it/s]\u001b[A\n",
            " 93% 28/30 [00:21<00:00,  3.73it/s]\u001b[A\n",
            " 97% 29/30 [00:21<00:00,  4.21it/s]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 3.118440866470337, 'eval_precision': 0.0016085761293361802, 'eval_recall': 0.30111111111111116, 'eval_ExactMatch': 0.0, 'eval_runtime': 22.4271, 'eval_samples_per_second': 1.338, 'eval_steps_per_second': 1.338, 'epoch': 0.5}\n",
            " 50% 600/1200 [50:50<11:21,  1.14s/it]\n",
            "100% 30/30 [00:21<00:00,  4.64it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{'loss': 3.1146, 'grad_norm': 88.08606719970703, 'learning_rate': 8.116666666666666e-06, 'epoch': 0.6}\n",
            " 60% 720/1200 [1:00:36<08:52,  1.11s/it]\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:12,  2.23it/s]\u001b[A\n",
            " 10% 3/30 [00:01<00:09,  2.83it/s]\u001b[A\n",
            " 13% 4/30 [00:01<00:11,  2.36it/s]\u001b[A\n",
            " 17% 5/30 [00:02<00:10,  2.32it/s]\u001b[A\n",
            " 20% 6/30 [00:02<00:10,  2.23it/s]\u001b[A\n",
            " 23% 7/30 [00:02<00:09,  2.34it/s]\u001b[A\n",
            " 27% 8/30 [00:03<00:12,  1.83it/s]\u001b[A\n",
            " 30% 9/30 [00:04<00:11,  1.82it/s]\u001b[A\n",
            " 33% 10/30 [00:04<00:10,  1.96it/s]\u001b[A\n",
            " 37% 11/30 [00:05<00:10,  1.82it/s]\u001b[A\n",
            " 40% 12/30 [00:06<00:14,  1.22it/s]\u001b[A\n",
            " 43% 13/30 [00:08<00:18,  1.11s/it]\u001b[A\n",
            " 47% 14/30 [00:09<00:16,  1.03s/it]\u001b[A\n",
            " 50% 15/30 [00:09<00:13,  1.13it/s]\u001b[A\n",
            " 53% 16/30 [00:11<00:14,  1.02s/it]\u001b[A\n",
            " 57% 17/30 [00:13<00:17,  1.31s/it]\u001b[A\n",
            " 60% 18/30 [00:14<00:14,  1.21s/it]\u001b[A\n",
            " 63% 19/30 [00:18<00:22,  2.01s/it]\u001b[A\n",
            " 67% 20/30 [00:19<00:19,  1.92s/it]\u001b[A\n",
            " 70% 21/30 [00:20<00:12,  1.40s/it]\u001b[A\n",
            " 73% 22/30 [00:20<00:08,  1.02s/it]\u001b[A\n",
            " 77% 23/30 [00:20<00:05,  1.30it/s]\u001b[A\n",
            " 80% 24/30 [00:20<00:03,  1.71it/s]\u001b[A\n",
            " 83% 25/30 [00:20<00:02,  2.21it/s]\u001b[A\n",
            " 87% 26/30 [00:20<00:01,  2.71it/s]\u001b[A\n",
            " 90% 27/30 [00:21<00:00,  3.22it/s]\u001b[A\n",
            " 93% 28/30 [00:21<00:00,  3.70it/s]\u001b[A\n",
            " 97% 29/30 [00:21<00:00,  4.17it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 3.0295166969299316, 'eval_precision': 0.001777086320530694, 'eval_recall': 0.3288888888888889, 'eval_ExactMatch': 0.0, 'eval_runtime': 22.4328, 'eval_samples_per_second': 1.337, 'eval_steps_per_second': 1.337, 'epoch': 0.6}\n",
            " 60% 720/1200 [1:00:58<08:52,  1.11s/it]\n",
            "100% 30/30 [00:21<00:00,  4.61it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "{'loss': 3.0091, 'grad_norm': 80.09330749511719, 'learning_rate': 6.116666666666668e-06, 'epoch': 0.7}\n",
            " 70% 840/1200 [1:10:43<06:57,  1.16s/it]\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:12,  2.24it/s]\u001b[A\n",
            " 10% 3/30 [00:01<00:09,  2.83it/s]\u001b[A\n",
            " 13% 4/30 [00:01<00:11,  2.36it/s]\u001b[A\n",
            " 17% 5/30 [00:02<00:10,  2.32it/s]\u001b[A\n",
            " 20% 6/30 [00:02<00:10,  2.21it/s]\u001b[A\n",
            " 23% 7/30 [00:02<00:09,  2.34it/s]\u001b[A\n",
            " 27% 8/30 [00:03<00:12,  1.83it/s]\u001b[A\n",
            " 30% 9/30 [00:04<00:11,  1.83it/s]\u001b[A\n",
            " 33% 10/30 [00:04<00:10,  1.97it/s]\u001b[A\n",
            " 37% 11/30 [00:05<00:10,  1.84it/s]\u001b[A\n",
            " 40% 12/30 [00:06<00:14,  1.23it/s]\u001b[A\n",
            " 43% 13/30 [00:08<00:18,  1.10s/it]\u001b[A\n",
            " 47% 14/30 [00:09<00:16,  1.03s/it]\u001b[A\n",
            " 50% 15/30 [00:09<00:13,  1.13it/s]\u001b[A\n",
            " 53% 16/30 [00:11<00:14,  1.03s/it]\u001b[A\n",
            " 57% 17/30 [00:13<00:17,  1.32s/it]\u001b[A\n",
            " 60% 18/30 [00:14<00:14,  1.22s/it]\u001b[A\n",
            " 63% 19/30 [00:18<00:22,  2.02s/it]\u001b[A\n",
            " 67% 20/30 [00:19<00:19,  1.93s/it]\u001b[A\n",
            " 70% 21/30 [00:20<00:12,  1.40s/it]\u001b[A\n",
            " 73% 22/30 [00:20<00:08,  1.02s/it]\u001b[A\n",
            " 77% 23/30 [00:20<00:05,  1.30it/s]\u001b[A\n",
            " 80% 24/30 [00:20<00:03,  1.71it/s]\u001b[A\n",
            " 83% 25/30 [00:20<00:02,  2.22it/s]\u001b[A\n",
            " 87% 26/30 [00:20<00:01,  2.73it/s]\u001b[A\n",
            " 90% 27/30 [00:21<00:00,  3.25it/s]\u001b[A\n",
            " 93% 28/30 [00:21<00:00,  3.73it/s]\u001b[A\n",
            " 97% 29/30 [00:21<00:00,  4.22it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 2.973583936691284, 'eval_precision': 0.001777086320530694, 'eval_recall': 0.3288888888888889, 'eval_ExactMatch': 0.0, 'eval_runtime': 22.4269, 'eval_samples_per_second': 1.338, 'eval_steps_per_second': 1.338, 'epoch': 0.7}\n",
            " 70% 840/1200 [1:11:06<06:57,  1.16s/it]\n",
            "100% 30/30 [00:21<00:00,  4.66it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            " 79% 945/1200 [1:19:47<36:23,  8.56s/it]"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Searching"
      ],
      "metadata": {
        "id": "PYI-311SxfRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import subprocess\n",
        "import optuna\n",
        "\n",
        "def objective(trial):\n",
        "    # Define your hyperparameters to tune\n",
        "    parameters = {\n",
        "      'batch_size':trial.suggest_int('batch_size', 1, 2),\n",
        "      'learning_rate':trial.suggest_float('learning_rate', 2e-5, 3e-4),\n",
        "      'gradient_accumulation_steps':trial.suggest_int('gradient_accumulation_steps', 1, 10),\n",
        "      'torch_empty_cache_steps':trial.suggest_int('torch_empty_cache_steps', 1, 10),\n",
        "      'peft_r':trial.suggest_int('peft_r', 0, 1024),\n",
        "      'fp16':trial.suggest_categorical('fp16', [True, False]),\n",
        "      'load_in_4bit':trial.suggest_categorical('load_in_4bit', [False, True]),\n",
        "      'optim':trial.suggest_categorical('optim', ['adamw_torch','adamw_bnb_8bit','adafactor']),\n",
        "      'weight_decay':trial.suggest_float('weight_decay', 0.01, 0.1),\n",
        "      'model_name':trial.suggest_categorical('model_name',\n",
        "      [\"meta-llama/Llama-3.2-1B-Instruct\",\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        \"Qwen/Qwen2.5-1.5B-Instruct\",\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
        "      ),\n",
        "    }\n",
        "    if parameters['peft_r']!=0:\n",
        "      parameters['peft_lora_alpha'] = trial.suggest_int('peft_lora_alpha', 0, 1024)\n",
        "    if parameters['fp16']:\n",
        "      parameters['fp16_opt_level'] = trial.suggest_categorical('fp16_opt_level', ['O0','O1','O2','O3'])\n",
        "    if not parameters['load_in_4bit']:\n",
        "      parameters['load_in_8bit'] = trial.suggest_categorical('load_in_8bit', [False, True])\n",
        "    if parameters['load_in_4bit'] or parameters['load_in_8bit']:\n",
        "      parameters[\"llm_int8_has_fp16_weight\"] = trial.suggest_categorical('llm_int8_has_fp16_weight', [False, True])\n",
        "      parameters[\"bnb_4bit_compute_dtype\"] = trial.suggest_categorical('bnb_4bit_compute_dtype', ['fp32','bf16'])\n",
        "      parameters[\"bnb_4bit_quant_type\"] = trial.suggest_categorical('bnb_4bit_quant_type', ['fp4','nf4'])\n",
        "      parameters['bnb_4bit_use_double_quant'] = trial.suggest_categorical('bnb_4bit_use_double_quant', [False, True])\n",
        "\n",
        "    command = [\"python\", \"/content/drive/MyDrive/models/training_simpleqa.py\",]\n",
        "    for key, value in parameters.items():\n",
        "      command.append(\n",
        "          f\"--{key}={str(value)}\"\n",
        "      )\n",
        "    joined_command = \" \".join(command)\n",
        "    print(f\"command is: {joined_command}\")\n",
        "    metric_inf = {}\n",
        "    try:\n",
        "        process = subprocess.Popen(\n",
        "            command, stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT, text=True, bufsize=1\n",
        "        )\n",
        "        captured_output = []\n",
        "        metric_inf_str = \"\"\n",
        "        with process.stdout:\n",
        "            for line in iter(process.stdout.readline, ''):\n",
        "                captured_output.append(line)\n",
        "                print(f\"   {line}\")\n",
        "                if line.find('eval_loss')!=-1 and line.find('eval_precision')!=-1 and line.find('eval_recall')!=-1:\n",
        "                    metric_inf_str = line\n",
        "        # process.communicate()\n",
        "        metric_inf_str = metric_inf_str.strip()\n",
        "        if metric_inf_str!='':\n",
        "            metric_inf = ast.literal_eval(metric_inf_str)\n",
        "            # eval_loss eval_precision eval_recall\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        # metric_inf['eval_loss'] = 9999999999999999999\n",
        "        raise optuna.TrialPruned(f\"Trial pruned due to error: {e}\")\n",
        "    if 'eval_loss' not in metric_inf:\n",
        "      raise optuna.TrialPruned(f\"Trial pruned due to error: no eval_loss\")\n",
        "\n",
        "    return metric_inf['eval_loss'], metric_inf['eval_samples_per_second'] #, metric_inf['eval_precision']"
      ],
      "metadata": {
        "id": "E4Xmxs_POVsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a study and optimize it\n",
        "storage = optuna.storages.RDBStorage(url=\"sqlite:////content/drive/MyDrive/models/optuna_trials.db\", heartbeat_interval=60, grace_period=120)\n",
        "study = optuna.create_study(\n",
        "    study_name='financialRAG_doublemin',\n",
        "    storage=storage,\n",
        "    load_if_exists=True,\n",
        "    directions=['minimize','minimize']\n",
        ")\n",
        "study.set_user_attr('contributors', ['TJ'])\n",
        "optuna.logging.set_verbosity(optuna.logging.DEBUG)\n",
        "study.optimize(objective, n_trials=300)\n",
        "\n",
        "print(\"Best hyperparameters:\", study.best_params)"
      ],
      "metadata": {
        "id": "ZjlffFt9mY0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e063125-ab0b-4449-e343-53bf8c359aba",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-07 05:43:11,057] Using an existing study with name 'financialRAG_doublemin' instead of creating a new one.\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n",
            "[D 2024-11-07 05:43:11,135] Trial 41 popped from the trial queue.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=0.00020715556360024592 --gradient_accumulation_steps=8 --torch_empty_cache_steps=6 --peft_r=997 --fp16=True --load_in_4bit=False --optim=adamw_bnb_8bit --weight_decay=0.08244680286182575 --model_name=Qwen/Qwen2.5-1.5B-Instruct --peft_lora_alpha=796 --fp16_opt_level=O3 --load_in_8bit=False\n",
            "   2024-11-07 05:43:21.475250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 05:43:21.708309: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 05:43:21.774771: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 05:43:22.151006: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 05:43:23.822380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 135786498 || all params: 1679503876 || trainable%: 8.08\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:05,  5.61 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:04,  6.62 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 10.79 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:01, 12.24 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:01, 12.25 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 12.84 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01, 10.13 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.37 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.20 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.17 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:01,  7.21 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:02,  5.49 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.40 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 10.70 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 15.70 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.13 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [00:15<30:19, 15.29s/it]\n",
            "\n",
            "     2%|▏         | 2/120 [00:31<30:46, 15.65s/it]\n",
            "\n",
            "     2%|▎         | 3/120 [00:47<30:44, 15.76s/it]\n",
            "\n",
            "     3%|▎         | 4/120 [01:00<28:44, 14.87s/it]\n",
            "\n",
            "     4%|▍         | 5/120 [01:18<30:48, 16.07s/it]\n",
            "\n",
            "     5%|▌         | 6/120 [02:18<58:32, 30.81s/it]\n",
            "\n",
            "     6%|▌         | 7/120 [03:11<1:11:56, 38.20s/it]\n",
            "\n",
            "     7%|▋         | 8/120 [04:10<1:23:25, 44.69s/it]\n",
            "\n",
            "     8%|▊         | 9/120 [04:53<1:21:47, 44.21s/it]\n",
            "\n",
            "     8%|▊         | 10/120 [06:07<1:38:15, 53.60s/it]\n",
            "\n",
            "     9%|▉         | 11/120 [06:12<1:09:54, 38.48s/it]\n",
            "\n",
            "    10%|█         | 12/120 [06:16<50:36, 28.11s/it]  \n",
            "\n",
            "    11%|█         | 13/120 [06:21<37:36, 21.09s/it]\n",
            "\n",
            "    12%|█▏        | 14/120 [06:26<28:24, 16.08s/it]\n",
            "\n",
            "    12%|█▎        | 15/120 [06:30<21:54, 12.52s/it]\n",
            "\n",
            "    13%|█▎        | 16/120 [06:44<22:43, 13.11s/it]\n",
            "\n",
            "    14%|█▍        | 17/120 [07:02<25:05, 14.62s/it]\n",
            "\n",
            "    15%|█▌        | 18/120 [07:20<26:23, 15.52s/it]\n",
            "\n",
            "    16%|█▌        | 19/120 [07:35<25:51, 15.36s/it]\n",
            "\n",
            "    17%|█▋        | 20/120 [07:54<27:35, 16.55s/it]\n",
            "\n",
            "    18%|█▊        | 21/120 [08:55<49:17, 29.88s/it]\n",
            "\n",
            "    18%|█▊        | 22/120 [09:49<1:00:16, 36.90s/it]\n",
            "\n",
            "    19%|█▉        | 23/120 [10:48<1:10:23, 43.54s/it]\n",
            "\n",
            "    20%|██        | 24/120 [11:31<1:09:31, 43.45s/it]\n",
            "\n",
            "    21%|██        | 25/120 [12:45<1:23:32, 52.77s/it]\n",
            "\n",
            "    22%|██▏       | 26/120 [12:50<59:51, 38.21s/it]  \n",
            "\n",
            "    22%|██▎       | 27/120 [12:54<43:32, 28.10s/it]\n",
            "\n",
            "    23%|██▎       | 28/120 [12:59<32:20, 21.10s/it]\n",
            "\n",
            "    24%|██▍       | 29/120 [13:03<24:27, 16.12s/it]\n",
            "\n",
            "    25%|██▌       | 30/120 [13:08<18:51, 12.57s/it]\n",
            "\n",
            "    26%|██▌       | 31/120 [13:22<19:33, 13.19s/it]\n",
            "\n",
            "    27%|██▋       | 32/120 [13:40<21:32, 14.69s/it]\n",
            "\n",
            "    28%|██▊       | 33/120 [13:58<22:31, 15.54s/it]\n",
            "\n",
            "    28%|██▊       | 34/120 [14:13<21:57, 15.31s/it]\n",
            "\n",
            "    29%|██▉       | 35/120 [14:32<23:23, 16.51s/it]\n",
            "\n",
            "    30%|███       | 36/120 [15:33<41:36, 29.72s/it]\n",
            "\n",
            "    31%|███       | 37/120 [16:26<50:53, 36.79s/it]\n",
            "\n",
            "    32%|███▏      | 38/120 [17:25<59:17, 43.38s/it]\n",
            "\n",
            "    32%|███▎      | 39/120 [18:08<58:32, 43.36s/it]\n",
            "\n",
            "    33%|███▎      | 40/120 [19:22<1:10:07, 52.60s/it]\n",
            "\n",
            "    34%|███▍      | 41/120 [19:26<50:08, 38.08s/it]  \n",
            "\n",
            "    35%|███▌      | 42/120 [19:31<36:24, 28.00s/it]\n",
            "\n",
            "    36%|███▌      | 43/120 [19:36<27:02, 21.07s/it]\n",
            "\n",
            "    37%|███▋      | 44/120 [19:40<20:23, 16.10s/it]\n",
            "\n",
            "    38%|███▊      | 45/120 [19:44<15:40, 12.54s/it]\n",
            "\n",
            "    38%|███▊      | 46/120 [19:59<16:10, 13.11s/it]\n",
            "\n",
            "    39%|███▉      | 47/120 [20:17<17:47, 14.62s/it]\n",
            "\n",
            "    40%|████      | 48/120 [20:35<18:35, 15.49s/it]\n",
            "\n",
            "    41%|████      | 49/120 [20:49<18:07, 15.32s/it]\n",
            "\n",
            "    42%|████▏     | 50/120 [21:09<19:16, 16.52s/it]\n",
            "\n",
            "    42%|████▎     | 51/120 [22:10<34:18, 29.83s/it]\n",
            "\n",
            "    43%|████▎     | 52/120 [23:03<41:43, 36.82s/it]\n",
            "\n",
            "    44%|████▍     | 53/120 [24:02<48:29, 43.42s/it]\n",
            "\n",
            "    45%|████▌     | 54/120 [24:45<47:45, 43.41s/it]\n",
            "\n",
            "    46%|████▌     | 55/120 [26:00<57:13, 52.82s/it]\n",
            "\n",
            "    47%|████▋     | 56/120 [26:04<40:48, 38.25s/it]\n",
            "\n",
            "    48%|████▊     | 57/120 [26:09<29:32, 28.13s/it]\n",
            "\n",
            "    48%|████▊     | 58/120 [26:13<21:46, 21.08s/it]\n",
            "\n",
            "    49%|████▉     | 59/120 [26:18<16:22, 16.10s/it]\n",
            "\n",
            "    50%|█████     | 60/120 [26:22<12:35, 12.60s/it]\n",
            "\n",
            "    51%|█████     | 61/120 [26:37<12:57, 13.17s/it]\n",
            "\n",
            "    52%|█████▏    | 62/120 [26:55<14:11, 14.68s/it]\n",
            "\n",
            "    52%|█████▎    | 63/120 [27:12<14:45, 15.53s/it]\n",
            "\n",
            "    53%|█████▎    | 64/120 [27:27<14:16, 15.30s/it]\n",
            "\n",
            "    54%|█████▍    | 65/120 [27:46<15:07, 16.50s/it]\n",
            "\n",
            "    55%|█████▌    | 66/120 [28:47<26:47, 29.77s/it]\n",
            "\n",
            "    56%|█████▌    | 67/120 [29:40<32:32, 36.83s/it]\n",
            "\n",
            "    57%|█████▋    | 68/120 [30:39<37:29, 43.25s/it]\n",
            "\n",
            "    57%|█████▊    | 69/120 [31:22<36:47, 43.28s/it]\n",
            "\n",
            "    58%|█████▊    | 70/120 [32:36<43:51, 52.63s/it]\n",
            "\n",
            "    59%|█████▉    | 71/120 [32:41<31:07, 38.11s/it]\n",
            "\n",
            "    60%|██████    | 72/120 [32:45<22:25, 28.02s/it]\n",
            "\n",
            "    61%|██████    | 73/120 [32:50<16:29, 21.05s/it]\n",
            "\n",
            "    62%|██████▏   | 74/120 [32:54<12:19, 16.09s/it]\n",
            "\n",
            "    62%|██████▎   | 75/120 [32:59<09:26, 12.58s/it]\n",
            "\n",
            "    63%|██████▎   | 76/120 [33:13<09:36, 13.11s/it]\n",
            "\n",
            "    64%|██████▍   | 77/120 [33:31<10:29, 14.63s/it]\n",
            "\n",
            "    65%|██████▌   | 78/120 [33:49<10:51, 15.50s/it]\n",
            "\n",
            "    66%|██████▌   | 79/120 [34:04<10:29, 15.35s/it]\n",
            "\n",
            "    67%|██████▋   | 80/120 [34:23<11:01, 16.55s/it]\n",
            "\n",
            "    68%|██████▊   | 81/120 [35:24<19:22, 29.81s/it]\n",
            "\n",
            "    68%|██████▊   | 82/120 [36:17<23:18, 36.81s/it]\n",
            "\n",
            "    69%|██████▉   | 83/120 [37:15<26:35, 43.13s/it]\n",
            "\n",
            "    70%|███████   | 84/120 [37:59<25:56, 43.25s/it]\n",
            "\n",
            "    71%|███████   | 85/120 [39:13<30:39, 52.55s/it]\n",
            "\n",
            "    72%|███████▏  | 86/120 [39:17<21:33, 38.05s/it]\n",
            "\n",
            "    72%|███████▎  | 87/120 [39:22<15:23, 27.98s/it]\n",
            "\n",
            "    73%|███████▎  | 88/120 [39:26<11:11, 20.97s/it]\n",
            "\n",
            "    74%|███████▍  | 89/120 [39:31<08:17, 16.03s/it]\n",
            "\n",
            "    75%|███████▌  | 90/120 [39:35<06:15, 12.51s/it]\n",
            "\n",
            "    76%|███████▌  | 91/120 [39:50<06:21, 13.14s/it]\n",
            "\n",
            "    77%|███████▋  | 92/120 [40:08<06:50, 14.67s/it]\n",
            "\n",
            "    78%|███████▊  | 93/120 [40:25<06:59, 15.52s/it]\n",
            "\n",
            "    78%|███████▊  | 94/120 [40:40<06:37, 15.31s/it]\n",
            "\n",
            "    79%|███████▉  | 95/120 [40:59<06:52, 16.51s/it]\n",
            "\n",
            "    80%|████████  | 96/120 [42:00<11:53, 29.74s/it]\n",
            "\n",
            "    81%|████████  | 97/120 [42:53<14:06, 36.81s/it]\n",
            "\n",
            "    82%|████████▏ | 98/120 [43:52<15:53, 43.32s/it]\n",
            "\n",
            "    82%|████████▎ | 99/120 [44:35<15:09, 43.30s/it]\n",
            "\n",
            "    83%|████████▎ | 100/120 [45:50<17:32, 52.63s/it]\n",
            "\n",
            "    84%|████████▍ | 101/120 [45:54<12:04, 38.12s/it]\n",
            "\n",
            "    85%|████████▌ | 102/120 [45:58<08:24, 28.02s/it]\n",
            "\n",
            "    86%|████████▌ | 103/120 [46:03<05:58, 21.09s/it]\n",
            "\n",
            "    87%|████████▋ | 104/120 [46:08<04:17, 16.11s/it]\n",
            "\n",
            "    88%|████████▊ | 105/120 [46:12<03:08, 12.56s/it]\n",
            "\n",
            "    88%|████████▊ | 106/120 [46:26<03:03, 13.11s/it]\n",
            "\n",
            "    89%|████████▉ | 107/120 [46:44<03:09, 14.60s/it]\n",
            "\n",
            "    90%|█████████ | 108/120 [47:02<03:06, 15.56s/it]\n",
            "\n",
            "    91%|█████████ | 109/120 [47:17<02:49, 15.37s/it]\n",
            "\n",
            "    92%|█████████▏| 110/120 [47:36<02:45, 16.55s/it]\n",
            "\n",
            "    92%|█████████▎| 111/120 [48:37<04:28, 29.84s/it]\n",
            "\n",
            "    93%|█████████▎| 112/120 [49:30<04:54, 36.85s/it]\n",
            "\n",
            "    94%|█████████▍| 113/120 [50:29<05:03, 43.37s/it]\n",
            "\n",
            "    95%|█████████▌| 114/120 [51:12<04:20, 43.37s/it]\n",
            "\n",
            "    96%|█████████▌| 115/120 [52:27<04:23, 52.76s/it]\n",
            "\n",
            "    97%|█████████▋| 116/120 [52:31<02:32, 38.20s/it]\n",
            "\n",
            "    98%|█████████▊| 117/120 [52:36<01:24, 28.09s/it]\n",
            "\n",
            "    98%|█████████▊| 118/120 [52:40<00:42, 21.05s/it]\n",
            "\n",
            "    99%|█████████▉| 119/120 [52:45<00:16, 16.10s/it]\n",
            "\n",
            "   100%|██████████| 120/120 [52:49<00:00, 12.55s/it]\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 120/120 [52:49<00:00, 12.55s/it]{'loss': 4.4472, 'grad_norm': 35.851112365722656, 'learning_rate': 1.2084074543347679e-05, 'epoch': 1.0}\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "     7%|▋         | 2/30 [00:02<00:37,  1.34s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    10%|█         | 3/30 [00:03<00:29,  1.09s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    13%|█▎        | 4/30 [00:05<00:34,  1.32s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    17%|█▋        | 5/30 [00:06<00:33,  1.34s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    20%|██        | 6/30 [00:07<00:33,  1.38s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    23%|██▎       | 7/30 [00:09<00:30,  1.31s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    27%|██▋       | 8/30 [00:11<00:36,  1.64s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    30%|███       | 9/30 [00:13<00:35,  1.67s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    33%|███▎      | 10/30 [00:14<00:31,  1.56s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    37%|███▋      | 11/30 [00:16<00:31,  1.66s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    40%|████      | 12/30 [00:20<00:44,  2.48s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    43%|████▎     | 13/30 [00:26<00:56,  3.34s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    47%|████▋     | 14/30 [00:28<00:49,  3.11s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    50%|█████     | 15/30 [00:30<00:40,  2.69s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    53%|█████▎    | 16/30 [00:34<00:43,  3.11s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    57%|█████▋    | 17/30 [00:40<00:51,  3.96s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    60%|██████    | 18/30 [00:43<00:43,  3.64s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    63%|██████▎   | 19/30 [00:54<01:04,  5.90s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    67%|██████▋   | 20/30 [00:59<00:56,  5.67s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    70%|███████   | 21/30 [01:00<00:37,  4.14s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    73%|███████▎  | 22/30 [01:00<00:24,  3.04s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    77%|███████▋  | 23/30 [01:01<00:16,  2.30s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    80%|████████  | 24/30 [01:01<00:10,  1.76s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    83%|████████▎ | 25/30 [01:02<00:06,  1.37s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    87%|████████▋ | 26/30 [01:02<00:04,  1.13s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    90%|█████████ | 27/30 [01:03<00:02,  1.04it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    93%|█████████▎| 28/30 [01:03<00:01,  1.19it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    97%|█████████▋| 29/30 [01:04<00:00,  1.33it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 30/30 [01:05<00:00,  1.41it/s]\u001b[A\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "                                                  \n",
            "\n",
            "   \u001b[A\n",
            "\n",
            "   100%|██████████| 120/120 [53:57<00:00, 12.55s/it]\n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 30/30 [01:05<00:00,  1.41it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "                                                  \u001b[A\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 120/120 [54:01<00:00, 12.55s/it]\n",
            "\n",
            "   100%|██████████| 120/120 [54:01<00:00, 27.02s/it]\n",
            "\n",
            "   {'eval_loss': 2.822599172592163, 'eval_precision': 0.06837026177662464, 'eval_recall': 0.4111111111111111, 'eval_ExactMatch': 0.066650390625, 'eval_runtime': 67.9458, 'eval_samples_per_second': 0.442, 'eval_steps_per_second': 0.442, 'epoch': 1.0}\n",
            "\n",
            "   {'train_runtime': 3242.147, 'train_samples_per_second': 0.296, 'train_steps_per_second': 0.037, 'train_loss': 4.447154235839844, 'epoch': 1.0}\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-07 06:39:12,351] Trial 41 finished with values: [2.822599172592163, 0.442] and parameters: {'batch_size': 1, 'learning_rate': 0.00020715556360024592, 'gradient_accumulation_steps': 8, 'torch_empty_cache_steps': 6, 'peft_r': 997, 'fp16': True, 'load_in_4bit': False, 'optim': 'adamw_bnb_8bit', 'weight_decay': 0.08244680286182575, 'model_name': 'Qwen/Qwen2.5-1.5B-Instruct', 'peft_lora_alpha': 796, 'fp16_opt_level': 'O3', 'load_in_8bit': False}.\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=2 --learning_rate=0.00014778501823750886 --gradient_accumulation_steps=8 --torch_empty_cache_steps=2 --peft_r=135 --fp16=False --load_in_4bit=False --optim=adamw_torch --weight_decay=0.020316957796791307 --model_name=Qwen/Qwen2.5-0.5B-Instruct --peft_lora_alpha=416 --load_in_8bit=True --llm_int8_has_fp16_weight=True --bnb_4bit_compute_dtype=bf16 --bnb_4bit_quant_type=nf4 --bnb_4bit_use_double_quant=False\n",
            "   2024-11-07 06:39:19.318961: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 06:39:19.346324: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 06:39:19.353174: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 06:39:19.369494: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 06:39:21.345016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 199, in <module>\n",
            "\n",
            "       model = AutoModelForQuestionAnswering.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "\n",
            "       return model_class.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4303, in from_pretrained\n",
            "\n",
            "       dispatch_model(model, **device_map_kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py\", line 352, in dispatch_model\n",
            "\n",
            "       check_device_map(model, device_map)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\", line 1407, in check_device_map\n",
            "\n",
            "       all_model_tensors = [name for name, _ in model.state_dict().items()]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2219, in state_dict\n",
            "\n",
            "       module.state_dict(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2219, in state_dict\n",
            "\n",
            "       module.state_dict(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2219, in state_dict\n",
            "\n",
            "       module.state_dict(\n",
            "\n",
            "     [Previous line repeated 2 more times]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2216, in state_dict\n",
            "\n",
            "       self._save_to_state_dict(destination, prefix, keep_vars)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\", line 928, in _save_to_state_dict\n",
            "\n",
            "       param_from_weight = getattr(self.weight, scb_name)\n",
            "\n",
            "   AttributeError: 'Tensor' object has no attribute 'SCB'\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-07 06:39:53,867] Trial 42 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=0.00017411068182595997 --gradient_accumulation_steps=5 --torch_empty_cache_steps=8 --peft_r=348 --fp16=False --load_in_4bit=False --optim=adafactor --weight_decay=0.01745211304446333 --model_name=Qwen/Qwen2.5-1.5B-Instruct --peft_lora_alpha=353 --load_in_8bit=False\n",
            "   2024-11-07 06:39:58.758153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 06:39:58.780480: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 06:39:58.787005: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 06:39:58.802312: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 06:39:59.949367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 47397890 || all params: 1591115268 || trainable%: 2.98\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:05,  4.86 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:05,  4.72 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:03,  7.37 examples/s]\n",
            "\n",
            "   Map:  17%|█▋        | 5/30 [00:00<00:03,  7.99 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:02,  8.50 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:01<00:02,  8.38 examples/s]\n",
            "\n",
            "   Map:  30%|███       | 9/30 [00:01<00:02,  8.46 examples/s]\n",
            "\n",
            "   Map:  37%|███▋      | 11/30 [00:01<00:02,  8.31 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:02,  6.38 examples/s]\n",
            "\n",
            "   Map:  43%|████▎     | 13/30 [00:02<00:03,  5.07 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:02<00:03,  5.15 examples/s]\n",
            "\n",
            "   Map:  50%|█████     | 15/30 [00:02<00:02,  5.60 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:02<00:02,  4.74 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:02<00:03,  4.01 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:03<00:02,  4.15 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:03<00:03,  3.29 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:03<00:02,  3.73 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:03<00:00,  8.47 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:04<00:00, 13.31 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:04<00:00,  7.18 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [00:10<20:15, 10.21s/it]\n",
            "\n",
            "     2%|▏         | 2/120 [00:20<19:46, 10.06s/it]\n",
            "\n",
            "     2%|▎         | 3/120 [00:30<19:40, 10.09s/it]\n",
            "\n",
            "     3%|▎         | 4/120 [00:41<20:05, 10.40s/it]\n",
            "\n",
            "     4%|▍         | 5/120 [00:52<20:24, 10.65s/it]\n",
            "\n",
            "     5%|▌         | 6/120 [00:58<17:39,  9.29s/it]\n",
            "\n",
            "     6%|▌         | 7/120 [01:12<20:18, 10.78s/it]\n",
            "\n",
            "     7%|▋         | 8/120 [01:22<19:18, 10.35s/it]\n",
            "\n",
            "     8%|▊         | 9/120 [02:07<39:36, 21.41s/it]\n",
            "\n",
            "     8%|▊         | 10/120 [02:27<38:23, 20.94s/it]\n",
            "\n",
            "     9%|▉         | 11/120 [03:07<48:21, 26.61s/it]\n",
            "\n",
            "    10%|█         | 12/120 [03:30<45:54, 25.50s/it]\n",
            "\n",
            "    11%|█         | 13/120 [04:11<54:04, 30.32s/it]\n",
            "\n",
            "    12%|█▏        | 14/120 [04:37<50:56, 28.83s/it]\n",
            "\n",
            "    12%|█▎        | 15/120 [05:05<50:25, 28.81s/it]\n",
            "\n",
            "    13%|█▎        | 16/120 [05:59<1:02:47, 36.23s/it]\n",
            "\n",
            "    14%|█▍        | 17/120 [06:01<44:44, 26.07s/it]  \n",
            "\n",
            "    15%|█▌        | 18/120 [06:04<32:19, 19.02s/it]\n",
            "\n",
            "    16%|█▌        | 19/120 [06:06<23:37, 14.04s/it]\n",
            "\n",
            "    17%|█▋        | 20/120 [06:09<17:41, 10.62s/it]\n",
            "\n",
            "    18%|█▊        | 21/120 [06:12<13:36,  8.24s/it]\n",
            "\n",
            "    18%|█▊        | 22/120 [06:14<10:39,  6.52s/it]\n",
            "\n",
            "    19%|█▉        | 23/120 [06:17<08:38,  5.34s/it]\n",
            "\n",
            "    20%|██        | 24/120 [06:19<07:09,  4.48s/it]\n",
            "\n",
            "    21%|██        | 25/120 [06:27<08:47,  5.56s/it]\n",
            "\n",
            "    22%|██▏       | 26/120 [06:37<10:46,  6.88s/it]\n",
            "\n",
            "    22%|██▎       | 27/120 [06:47<12:07,  7.82s/it]\n",
            "\n",
            "    23%|██▎       | 28/120 [06:57<13:06,  8.55s/it]\n",
            "\n",
            "    24%|██▍       | 29/120 [07:08<13:54,  9.17s/it]\n",
            "\n",
            "    25%|██▌       | 30/120 [07:15<12:38,  8.42s/it]\n",
            "\n",
            "    26%|██▌       | 31/120 [07:29<14:52, 10.03s/it]\n",
            "\n",
            "    27%|██▋       | 32/120 [07:38<14:24,  9.83s/it]\n",
            "\n",
            "    28%|██▊       | 33/120 [08:24<30:13, 20.84s/it]\n",
            "\n",
            "    28%|██▊       | 34/120 [08:44<29:29, 20.58s/it]\n",
            "\n",
            "    29%|██▉       | 35/120 [09:24<37:09, 26.23s/it]\n",
            "\n",
            "    30%|███       | 36/120 [09:47<35:29, 25.36s/it]\n",
            "\n",
            "    31%|███       | 37/120 [10:29<41:46, 30.20s/it]\n",
            "\n",
            "    32%|███▏      | 38/120 [10:54<39:15, 28.73s/it]\n",
            "\n",
            "    32%|███▎      | 39/120 [11:23<38:55, 28.83s/it]\n",
            "\n",
            "    33%|███▎      | 40/120 [12:16<48:13, 36.17s/it]\n",
            "\n",
            "    34%|███▍      | 41/120 [12:19<34:17, 26.04s/it]\n",
            "\n",
            "    35%|███▌      | 42/120 [12:21<24:43, 19.01s/it]\n",
            "\n",
            "    36%|███▌      | 43/120 [12:24<18:00, 14.03s/it]\n",
            "\n",
            "    37%|███▋      | 44/120 [12:26<13:24, 10.59s/it]\n",
            "\n",
            "    38%|███▊      | 45/120 [12:29<10:18,  8.24s/it]\n",
            "\n",
            "    38%|███▊      | 46/120 [12:32<08:03,  6.54s/it]\n",
            "\n",
            "    39%|███▉      | 47/120 [12:34<06:30,  5.35s/it]\n",
            "\n",
            "    40%|████      | 48/120 [12:37<05:23,  4.49s/it]\n",
            "\n",
            "    41%|████      | 49/120 [12:45<06:32,  5.53s/it]\n",
            "\n",
            "    42%|████▏     | 50/120 [12:55<07:59,  6.85s/it]\n",
            "\n",
            "    42%|████▎     | 51/120 [13:05<08:58,  7.80s/it]\n",
            "\n",
            "    43%|████▎     | 52/120 [13:15<09:41,  8.55s/it]\n",
            "\n",
            "    44%|████▍     | 53/120 [13:25<10:14,  9.17s/it]\n",
            "\n",
            "    45%|████▌     | 54/120 [13:32<09:15,  8.41s/it]\n",
            "\n",
            "    46%|████▌     | 55/120 [13:46<10:51, 10.02s/it]\n",
            "\n",
            "    47%|████▋     | 56/120 [13:55<10:28,  9.82s/it]\n",
            "\n",
            "    48%|████▊     | 57/120 [14:41<21:43, 20.69s/it]\n",
            "\n",
            "    48%|████▊     | 58/120 [15:01<21:05, 20.41s/it]\n",
            "\n",
            "    49%|████▉     | 59/120 [15:41<26:38, 26.21s/it]\n",
            "\n",
            "    50%|█████     | 60/120 [16:04<25:17, 25.29s/it]\n",
            "\n",
            "    51%|█████     | 61/120 [16:45<29:36, 30.10s/it]\n",
            "\n",
            "    52%|█████▏    | 62/120 [17:11<27:45, 28.71s/it]\n",
            "\n",
            "    52%|█████▎    | 63/120 [17:39<27:16, 28.71s/it]\n",
            "\n",
            "    53%|█████▎    | 64/120 [18:33<33:38, 36.04s/it]\n",
            "\n",
            "    54%|█████▍    | 65/120 [18:35<23:48, 25.97s/it]\n",
            "\n",
            "    55%|█████▌    | 66/120 [18:38<17:05, 18.99s/it]\n",
            "\n",
            "    56%|█████▌    | 67/120 [18:40<12:24, 14.04s/it]\n",
            "\n",
            "    57%|█████▋    | 68/120 [18:43<09:12, 10.62s/it]\n",
            "\n",
            "    57%|█████▊    | 69/120 [18:46<07:01,  8.26s/it]\n",
            "\n",
            "    58%|█████▊    | 70/120 [18:48<05:27,  6.54s/it]\n",
            "\n",
            "    59%|█████▉    | 71/120 [18:51<04:23,  5.37s/it]\n",
            "\n",
            "    60%|██████    | 72/120 [18:53<03:36,  4.51s/it]\n",
            "\n",
            "    61%|██████    | 73/120 [19:02<04:24,  5.62s/it]\n",
            "\n",
            "    62%|██████▏   | 74/120 [19:12<05:19,  6.95s/it]\n",
            "\n",
            "    62%|██████▎   | 75/120 [19:22<05:55,  7.90s/it]\n",
            "\n",
            "    63%|██████▎   | 76/120 [19:32<06:20,  8.65s/it]\n",
            "\n",
            "    64%|██████▍   | 77/120 [19:43<06:39,  9.29s/it]\n",
            "\n",
            "    65%|██████▌   | 78/120 [19:50<05:56,  8.49s/it]\n",
            "\n",
            "    66%|██████▌   | 79/120 [20:03<06:55, 10.12s/it]\n",
            "\n",
            "    67%|██████▋   | 80/120 [20:13<06:40, 10.00s/it]\n",
            "\n",
            "    68%|██████▊   | 81/120 [20:59<13:32, 20.83s/it]\n",
            "\n",
            "    68%|██████▊   | 82/120 [21:19<12:59, 20.51s/it]\n",
            "\n",
            "    69%|██████▉   | 83/120 [21:59<16:11, 26.25s/it]\n",
            "\n",
            "    70%|███████   | 84/120 [22:22<15:11, 25.33s/it]\n",
            "\n",
            "    71%|███████   | 85/120 [23:03<17:36, 30.18s/it]\n",
            "\n",
            "    72%|███████▏  | 86/120 [23:29<16:17, 28.75s/it]\n",
            "\n",
            "    72%|███████▎  | 87/120 [23:58<15:54, 28.91s/it]\n",
            "\n",
            "    73%|███████▎  | 88/120 [24:52<19:23, 36.35s/it]\n",
            "\n",
            "    74%|███████▍  | 89/120 [24:54<13:31, 26.18s/it]\n",
            "\n",
            "    75%|███████▌  | 90/120 [24:57<09:33, 19.11s/it]\n",
            "\n",
            "    76%|███████▌  | 91/120 [24:59<06:48, 14.10s/it]\n",
            "\n",
            "    77%|███████▋  | 92/120 [25:02<04:58, 10.65s/it]\n",
            "\n",
            "    78%|███████▊  | 93/120 [25:05<03:43,  8.28s/it]\n",
            "\n",
            "    78%|███████▊  | 94/120 [25:07<02:50,  6.55s/it]\n",
            "\n",
            "    79%|███████▉  | 95/120 [25:10<02:14,  5.36s/it]\n",
            "\n",
            "    80%|████████  | 96/120 [25:12<01:47,  4.49s/it]\n",
            "\n",
            "    81%|████████  | 97/120 [25:20<02:07,  5.56s/it]\n",
            "\n",
            "    82%|████████▏ | 98/120 [25:30<02:31,  6.89s/it]\n",
            "\n",
            "    82%|████████▎ | 99/120 [25:40<02:44,  7.83s/it]\n",
            "\n",
            "    83%|████████▎ | 100/120 [25:51<02:51,  8.57s/it]\n",
            "\n",
            "    84%|████████▍ | 101/120 [26:01<02:54,  9.19s/it]\n",
            "\n",
            "    85%|████████▌ | 102/120 [26:08<02:31,  8.44s/it]\n",
            "\n",
            "    86%|████████▌ | 103/120 [26:22<02:50, 10.06s/it]\n",
            "\n",
            "    87%|████████▋ | 104/120 [26:31<02:37,  9.86s/it]\n",
            "\n",
            "    88%|████████▊ | 105/120 [27:18<05:13, 20.92s/it]\n",
            "\n",
            "    88%|████████▊ | 106/120 [27:38<04:49, 20.68s/it]\n",
            "\n",
            "    89%|████████▉ | 107/120 [28:17<05:42, 26.33s/it]\n",
            "\n",
            "    90%|█████████ | 108/120 [28:41<05:05, 25.42s/it]\n",
            "\n",
            "    91%|█████████ | 109/120 [29:22<05:31, 30.15s/it]\n",
            "\n",
            "    92%|█████████▏| 110/120 [29:47<04:47, 28.76s/it]\n",
            "\n",
            "    92%|█████████▎| 111/120 [30:16<04:19, 28.79s/it]\n",
            "\n",
            "    93%|█████████▎| 112/120 [31:10<04:49, 36.15s/it]\n",
            "\n",
            "    94%|█████████▍| 113/120 [31:12<03:02, 26.08s/it]\n",
            "\n",
            "    95%|█████████▌| 114/120 [31:15<01:54, 19.15s/it]\n",
            "\n",
            "    96%|█████████▌| 115/120 [31:18<01:10, 14.15s/it]\n",
            "\n",
            "    97%|█████████▋| 116/120 [31:20<00:42, 10.72s/it]\n",
            "\n",
            "    98%|█████████▊| 117/120 [31:23<00:25,  8.34s/it]\n",
            "\n",
            "    98%|█████████▊| 118/120 [31:26<00:13,  6.60s/it]\n",
            "\n",
            "    99%|█████████▉| 119/120 [31:28<00:05,  5.41s/it]\n",
            "\n",
            "   100%|██████████| 120/120 [31:31<00:00,  4.55s/it]\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 120/120 [31:31<00:00,  4.55s/it]{'loss': 4.2912, 'grad_norm': 33.168704986572266, 'learning_rate': 1.0156456439847665e-05, 'epoch': 1.0}\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "     7%|▋         | 2/30 [00:02<00:35,  1.27s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    10%|█         | 3/30 [00:03<00:27,  1.03s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    13%|█▎        | 4/30 [00:04<00:32,  1.25s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    17%|█▋        | 5/30 [00:06<00:31,  1.26s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    20%|██        | 6/30 [00:07<00:31,  1.31s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    23%|██▎       | 7/30 [00:08<00:28,  1.24s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    27%|██▋       | 8/30 [00:10<00:34,  1.56s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    30%|███       | 9/30 [00:12<00:33,  1.60s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    33%|███▎      | 10/30 [00:13<00:29,  1.50s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    37%|███▋      | 11/30 [00:15<00:29,  1.57s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    40%|████      | 12/30 [00:19<00:42,  2.36s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    43%|████▎     | 13/30 [00:24<00:54,  3.19s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    47%|████▋     | 14/30 [00:27<00:47,  2.96s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    50%|█████     | 15/30 [00:28<00:38,  2.57s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    53%|█████▎    | 16/30 [00:32<00:41,  2.99s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    57%|█████▋    | 17/30 [00:38<00:49,  3.80s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    60%|██████    | 18/30 [00:41<00:41,  3.49s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    63%|██████▎   | 19/30 [00:52<01:02,  5.69s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    67%|██████▋   | 20/30 [00:57<00:54,  5.49s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    70%|███████   | 21/30 [00:57<00:35,  4.00s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    73%|███████▎  | 22/30 [00:58<00:23,  2.93s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    77%|███████▋  | 23/30 [00:58<00:15,  2.22s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    80%|████████  | 24/30 [00:59<00:10,  1.70s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    83%|████████▎ | 25/30 [00:59<00:06,  1.32s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    87%|████████▋ | 26/30 [01:00<00:04,  1.07s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    90%|█████████ | 27/30 [01:00<00:02,  1.09it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    93%|█████████▎| 28/30 [01:01<00:01,  1.24it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    97%|█████████▋| 29/30 [01:01<00:00,  1.41it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 30/30 [01:02<00:00,  1.50it/s]\u001b[A\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "                                                  \n",
            "\n",
            "   \u001b[A\n",
            "\n",
            "   100%|██████████| 120/120 [32:36<00:00,  4.55s/it]\n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 30/30 [01:02<00:00,  1.50it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "                                                  \u001b[A\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 120/120 [32:38<00:00,  4.55s/it]\n",
            "\n",
            "   100%|██████████| 120/120 [32:38<00:00, 16.32s/it]\n",
            "\n",
            "   {'eval_loss': 3.060774803161621, 'eval_precision': 0.0348781648890871, 'eval_recall': 0.34444444444444444, 'eval_ExactMatch': 0.0333251953125, 'eval_runtime': 64.8927, 'eval_samples_per_second': 0.462, 'eval_steps_per_second': 0.462, 'epoch': 1.0}\n",
            "\n",
            "   {'train_runtime': 1958.3152, 'train_samples_per_second': 0.306, 'train_steps_per_second': 0.061, 'train_loss': 4.291215515136718, 'epoch': 1.0}\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-07 07:13:06,571] Trial 43 finished with values: [3.060774803161621, 0.462] and parameters: {'batch_size': 1, 'learning_rate': 0.00017411068182595997, 'gradient_accumulation_steps': 5, 'torch_empty_cache_steps': 8, 'peft_r': 348, 'fp16': False, 'load_in_4bit': False, 'optim': 'adafactor', 'weight_decay': 0.01745211304446333, 'model_name': 'Qwen/Qwen2.5-1.5B-Instruct', 'peft_lora_alpha': 353, 'load_in_8bit': False}.\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=0.00024346046691409406 --gradient_accumulation_steps=3 --torch_empty_cache_steps=5 --peft_r=247 --fp16=False --load_in_4bit=True --optim=adamw_torch --weight_decay=0.04233237863481072 --model_name=Qwen/Qwen2.5-1.5B-Instruct --peft_lora_alpha=756 --llm_int8_has_fp16_weight=True --bnb_4bit_compute_dtype=bf16 --bnb_4bit_quant_type=nf4 --bnb_4bit_use_double_quant=False\n",
            "   2024-11-07 07:13:10.997897: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 07:13:11.018719: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 07:13:11.025362: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 07:13:11.041220: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 07:13:12.320476: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 33642498 || all params: 922262020 || trainable%: 3.65\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:04,  6.97 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:03,  7.92 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 12.00 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:01, 13.34 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:01, 12.98 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 13.15 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01, 10.18 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:02,  6.99 examples/s]\n",
            "\n",
            "   Map:  50%|█████     | 15/30 [00:01<00:02,  7.19 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:02,  6.14 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:02<00:02,  4.94 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:02,  4.85 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:03,  3.63 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:03<00:02,  3.57 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:03<00:00,  7.40 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:03<00:00, 10.87 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:03<00:00, 10.75 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:03<00:00,  7.93 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [00:19<38:50, 19.58s/it]\n",
            "\n",
            "     2%|▏         | 2/120 [00:47<48:14, 24.53s/it]\n",
            "\n",
            "     2%|▎         | 3/120 [01:13<48:49, 25.04s/it]\n",
            "\n",
            "     3%|▎         | 4/120 [01:36<47:07, 24.38s/it]\n",
            "\n",
            "     4%|▍         | 5/120 [01:58<45:05, 23.52s/it]\n",
            "\n",
            "     5%|▌         | 6/120 [02:23<45:19, 23.85s/it]\n",
            "\n",
            "     6%|▌         | 7/120 [02:47<45:20, 24.08s/it]\n",
            "\n",
            "     7%|▋         | 8/120 [03:19<49:17, 26.40s/it]\n",
            "\n",
            "     8%|▊         | 9/120 [03:42<47:01, 25.42s/it]\n",
            "\n",
            "     8%|▊         | 10/120 [03:54<39:04, 21.31s/it]\n",
            "\n",
            "     9%|▉         | 11/120 [04:27<45:29, 25.04s/it]\n",
            "\n",
            "    10%|█         | 12/120 [04:57<47:29, 26.39s/it]\n",
            "\n",
            "    11%|█         | 13/120 [05:26<48:29, 27.19s/it]Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3612, in training_step\n",
            "\n",
            "       self.accelerator.backward(loss, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2237, in backward\n",
            "\n",
            "       self.scaler.scale(loss).backward(**kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n",
            "\n",
            "       torch.autograd.backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "\n",
            "       _engine_run_backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "\n",
            "       return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 307, in apply\n",
            "\n",
            "       return user_fn(self, *args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 321, in backward\n",
            "\n",
            "       torch.autograd.backward(outputs_with_grad, args_with_grad)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "\n",
            "       _engine_run_backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "\n",
            "       return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 14.75 GiB of which 129.06 MiB is free. Process 315002 has 14.62 GiB memory in use. Of the allocated memory 12.56 GiB is allocated by PyTorch, and 1.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "    11%|█         | 13/120 [05:51<48:11, 27.02s/it]\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-07 07:19:23,317] Trial 44 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=2 --learning_rate=0.00025282169398484965 --gradient_accumulation_steps=2 --torch_empty_cache_steps=5 --peft_r=310 --fp16=True --load_in_4bit=True --optim=adafactor --weight_decay=0.039971510798161136 --model_name=meta-llama/Llama-3.2-1B-Instruct --peft_lora_alpha=826 --fp16_opt_level=O3 --llm_int8_has_fp16_weight=False --bnb_4bit_compute_dtype=fp32 --bnb_4bit_quant_type=fp4 --bnb_4bit_use_double_quant=False\n",
            "   2024-11-07 07:19:27.985852: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 07:19:28.007087: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 07:19:28.013615: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 07:19:28.029267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 07:19:29.209866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 33017858 || all params: 782297092 || trainable%: 4.22\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:04,  6.84 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:03,  7.28 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 10.29 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:02,  9.28 examples/s]\n",
            "\n",
            "   Map:  23%|██▎       | 7/30 [00:00<00:02,  9.26 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:02,  7.82 examples/s]\n",
            "\n",
            "   Map:  30%|███       | 9/30 [00:01<00:02,  7.68 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:01<00:02,  8.14 examples/s]\n",
            "\n",
            "   Map:  37%|███▋      | 11/30 [00:01<00:02,  8.01 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:02,  6.37 examples/s]\n",
            "\n",
            "   Map:  43%|████▎     | 13/30 [00:01<00:03,  5.26 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:02,  5.48 examples/s]\n",
            "\n",
            "   Map:  50%|█████     | 15/30 [00:02<00:02,  6.16 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:02<00:02,  5.58 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:02<00:02,  4.85 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:02,  5.05 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:03<00:03,  3.57 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:03<00:02,  3.56 examples/s]\n",
            "\n",
            "   Map:  77%|███████▋  | 23/30 [00:03<00:01,  6.61 examples/s]\n",
            "\n",
            "   Map:  87%|████████▋ | 26/30 [00:03<00:00,  9.50 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:03<00:00, 11.17 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:04<00:00, 12.83 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:04<00:00,  7.24 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/60 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     2%|▏         | 1/60 [00:58<57:31, 58.51s/it]\n",
            "\n",
            "     3%|▎         | 2/60 [01:46<50:36, 52.35s/it]Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3612, in training_step\n",
            "\n",
            "       self.accelerator.backward(loss, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2237, in backward\n",
            "\n",
            "       self.scaler.scale(loss).backward(**kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n",
            "\n",
            "       torch.autograd.backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "\n",
            "       _engine_run_backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "\n",
            "       return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 307, in apply\n",
            "\n",
            "       return user_fn(self, *args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 321, in backward\n",
            "\n",
            "       torch.autograd.backward(outputs_with_grad, args_with_grad)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "\n",
            "       _engine_run_backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "\n",
            "       return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 890.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 251.06 MiB is free. Process 335758 has 14.50 GiB memory in use. Of the allocated memory 12.28 GiB is allocated by PyTorch, and 2.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "     3%|▎         | 2/60 [02:00<58:05, 60.09s/it]\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-11-07 07:23:38,651] Trial 45 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=7.151863291549191e-05 --gradient_accumulation_steps=5 --torch_empty_cache_steps=6 --peft_r=983 --fp16=False --load_in_4bit=False --optim=adafactor --weight_decay=0.02717610521702047 --model_name=meta-llama/Llama-3.2-1B-Instruct --peft_lora_alpha=877 --load_in_8bit=False\n",
            "   2024-11-07 07:23:42.604317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 07:23:42.625033: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 07:23:42.631558: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 07:23:42.647867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 07:23:43.889486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 104689666 || all params: 1340508164 || trainable%: 7.81\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:04,  6.53 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:04,  6.98 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 10.73 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:02, 11.59 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:01, 11.31 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 11.24 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01, 10.01 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.86 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.90 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.96 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:01<00:01,  7.87 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  6.01 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  6.00 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 11.45 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 16.22 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 15.90 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.54 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [00:08<16:45,  8.45s/it]\n",
            "\n",
            "     2%|▏         | 2/120 [00:16<16:23,  8.34s/it]\n",
            "\n",
            "     2%|▎         | 3/120 [00:24<16:05,  8.25s/it]\n",
            "\n",
            "     3%|▎         | 4/120 [00:33<16:25,  8.49s/it]\n",
            "\n",
            "     4%|▍         | 5/120 [00:42<16:27,  8.59s/it]\n",
            "\n",
            "     5%|▌         | 6/120 [00:47<14:08,  7.44s/it]\n",
            "\n",
            "     6%|▌         | 7/120 [00:58<16:22,  8.69s/it]\n",
            "\n",
            "     7%|▋         | 8/120 [01:06<15:34,  8.34s/it]\n",
            "\n",
            "     8%|▊         | 9/120 [01:30<24:31, 13.26s/it]\n",
            "\n",
            "     8%|▊         | 10/120 [01:41<22:44, 12.40s/it]\n",
            "\n",
            "     9%|▉         | 11/120 [02:01<26:49, 14.76s/it]\n",
            "\n",
            "    10%|█         | 12/120 [02:13<25:16, 14.04s/it]\n",
            "\n",
            "    11%|█         | 13/120 [02:34<28:56, 16.23s/it]\n",
            "\n",
            "    12%|█▏        | 14/120 [02:47<26:50, 15.20s/it]\n",
            "\n",
            "    12%|█▎        | 15/120 [03:02<26:08, 14.94s/it]\n",
            "\n",
            "    13%|█▎        | 16/120 [03:27<31:28, 18.16s/it]\n",
            "\n",
            "    14%|█▍        | 17/120 [03:29<22:45, 13.25s/it]\n",
            "\n",
            "    15%|█▌        | 18/120 [03:31<16:46,  9.86s/it]\n",
            "\n",
            "    16%|█▌        | 19/120 [03:33<12:33,  7.46s/it]\n",
            "\n",
            "    17%|█▋        | 20/120 [03:35<09:39,  5.80s/it]\n",
            "\n",
            "    18%|█▊        | 21/120 [03:37<07:40,  4.66s/it]\n",
            "\n",
            "    18%|█▊        | 22/120 [03:39<06:14,  3.82s/it]\n",
            "\n",
            "    19%|█▉        | 23/120 [03:41<05:16,  3.26s/it]\n",
            "\n",
            "    20%|██        | 24/120 [03:43<04:36,  2.88s/it]\n",
            "\n",
            "    21%|██        | 25/120 [03:49<06:18,  3.98s/it]\n",
            "\n",
            "    22%|██▏       | 26/120 [03:57<08:09,  5.21s/it]\n",
            "\n",
            "    22%|██▎       | 27/120 [04:05<09:23,  6.06s/it]\n",
            "\n",
            "    23%|██▎       | 28/120 [04:14<10:21,  6.75s/it]\n",
            "\n",
            "    24%|██▍       | 29/120 [04:22<11:04,  7.30s/it]\n",
            "\n",
            "    25%|██▌       | 30/120 [04:27<09:59,  6.66s/it]\n",
            "\n",
            "    26%|██▌       | 31/120 [04:39<11:59,  8.09s/it]\n",
            "\n",
            "    27%|██▋       | 32/120 [04:47<11:46,  8.03s/it]\n",
            "\n",
            "    28%|██▊       | 33/120 [05:11<18:44, 12.92s/it]\n",
            "\n",
            "    28%|██▊       | 34/120 [05:22<17:33, 12.25s/it]\n",
            "\n",
            "    29%|██▉       | 35/120 [05:41<20:33, 14.51s/it]\n",
            "\n",
            "    30%|███       | 36/120 [05:54<19:26, 13.88s/it]\n",
            "\n",
            "    31%|███       | 37/120 [06:15<22:15, 16.09s/it]\n",
            "\n",
            "    32%|███▏      | 38/120 [06:28<20:38, 15.11s/it]\n",
            "\n",
            "    32%|███▎      | 39/120 [06:43<20:12, 14.97s/it]\n",
            "\n",
            "    33%|███▎      | 40/120 [07:09<24:23, 18.30s/it]\n",
            "\n",
            "    34%|███▍      | 41/120 [07:10<17:34, 13.34s/it]\n",
            "\n",
            "    35%|███▌      | 42/120 [07:12<12:55,  9.95s/it]\n",
            "\n",
            "    36%|███▌      | 43/120 [07:14<09:41,  7.55s/it]\n",
            "\n",
            "    37%|███▋      | 44/120 [07:16<07:26,  5.87s/it]\n",
            "\n",
            "    38%|███▊      | 45/120 [07:18<05:53,  4.71s/it]\n",
            "\n",
            "    38%|███▊      | 46/120 [07:20<04:45,  3.86s/it]\n",
            "\n",
            "    39%|███▉      | 47/120 [07:22<03:59,  3.28s/it]\n",
            "\n",
            "    40%|████      | 48/120 [07:24<03:25,  2.85s/it]\n",
            "\n",
            "    41%|████      | 49/120 [07:30<04:38,  3.93s/it]\n",
            "\n",
            "    42%|████▏     | 50/120 [07:39<06:01,  5.16s/it]\n",
            "\n",
            "    42%|████▎     | 51/120 [07:46<06:52,  5.99s/it]\n",
            "\n",
            "    43%|████▎     | 52/120 [07:55<07:34,  6.69s/it]\n",
            "\n",
            "    44%|████▍     | 53/120 [08:03<08:07,  7.27s/it]\n",
            "\n",
            "    45%|████▌     | 54/120 [08:08<07:17,  6.63s/it]\n",
            "\n",
            "    46%|████▌     | 55/120 [08:20<08:40,  8.01s/it]\n",
            "\n",
            "    47%|████▋     | 56/120 [08:27<08:25,  7.90s/it]\n",
            "\n",
            "    48%|████▊     | 57/120 [08:52<13:34, 12.93s/it]\n",
            "\n",
            "    48%|████▊     | 58/120 [09:03<12:37, 12.21s/it]\n",
            "\n",
            "    49%|████▉     | 59/120 [09:22<14:44, 14.50s/it]\n",
            "\n",
            "    50%|█████     | 60/120 [09:35<13:51, 13.85s/it]\n",
            "\n",
            "    51%|█████     | 61/120 [09:56<15:47, 16.06s/it]\n",
            "\n",
            "    52%|█████▏    | 62/120 [10:09<14:36, 15.12s/it]\n",
            "\n",
            "    52%|█████▎    | 63/120 [10:23<14:08, 14.89s/it]\n",
            "\n",
            "    53%|█████▎    | 64/120 [10:49<16:54, 18.12s/it]\n",
            "\n",
            "    54%|█████▍    | 65/120 [10:51<12:06, 13.22s/it]\n",
            "\n",
            "    55%|█████▌    | 66/120 [10:53<08:51,  9.84s/it]\n",
            "\n",
            "    56%|█████▌    | 67/120 [10:55<06:35,  7.46s/it]\n",
            "\n",
            "    57%|█████▋    | 68/120 [10:56<05:01,  5.80s/it]\n",
            "\n",
            "    57%|█████▊    | 69/120 [10:59<03:58,  4.67s/it]\n",
            "\n",
            "    58%|█████▊    | 70/120 [11:00<03:12,  3.85s/it]\n",
            "\n",
            "    59%|█████▉    | 71/120 [11:02<02:41,  3.29s/it]\n",
            "\n",
            "    60%|██████    | 72/120 [11:04<02:16,  2.85s/it]\n",
            "\n",
            "    61%|██████    | 73/120 [11:11<03:04,  3.93s/it]\n",
            "\n",
            "    62%|██████▏   | 74/120 [11:19<03:57,  5.17s/it]\n",
            "\n",
            "    62%|██████▎   | 75/120 [11:27<04:29,  5.99s/it]\n",
            "\n",
            "    63%|██████▎   | 76/120 [11:35<04:57,  6.75s/it]\n",
            "\n",
            "    64%|██████▍   | 77/120 [11:44<05:13,  7.29s/it]\n",
            "\n",
            "    65%|██████▌   | 78/120 [11:49<04:40,  6.68s/it]\n",
            "\n",
            "    66%|██████▌   | 79/120 [12:00<05:30,  8.06s/it]\n",
            "\n",
            "    67%|██████▋   | 80/120 [12:08<05:18,  7.97s/it]\n",
            "\n",
            "    68%|██████▊   | 81/120 [12:32<08:21, 12.86s/it]\n",
            "\n",
            "    68%|██████▊   | 82/120 [12:43<07:41, 12.16s/it]\n",
            "\n",
            "    69%|██████▉   | 83/120 [13:03<08:57, 14.52s/it]\n",
            "\n",
            "    70%|███████   | 84/120 [13:15<08:19, 13.87s/it]\n",
            "\n",
            "    71%|███████   | 85/120 [13:37<09:24, 16.13s/it]\n",
            "\n",
            "    72%|███████▏  | 86/120 [13:49<08:33, 15.12s/it]\n",
            "\n",
            "    72%|███████▎  | 87/120 [14:04<08:11, 14.90s/it]\n",
            "\n",
            "    73%|███████▎  | 88/120 [14:29<09:40, 18.14s/it]\n",
            "\n",
            "    74%|███████▍  | 89/120 [14:31<06:50, 13.25s/it]\n",
            "\n",
            "    75%|███████▌  | 90/120 [14:33<04:56,  9.87s/it]\n",
            "\n",
            "    76%|███████▌  | 91/120 [14:35<03:36,  7.48s/it]\n",
            "\n",
            "    77%|███████▋  | 92/120 [14:37<02:42,  5.81s/it]\n",
            "\n",
            "    78%|███████▊  | 93/120 [14:39<02:06,  4.67s/it]\n",
            "\n",
            "    78%|███████▊  | 94/120 [14:41<01:39,  3.83s/it]\n",
            "\n",
            "    79%|███████▉  | 95/120 [14:43<01:21,  3.27s/it]\n",
            "\n",
            "    80%|████████  | 96/120 [14:45<01:09,  2.88s/it]\n",
            "\n",
            "    81%|████████  | 97/120 [14:51<01:31,  3.99s/it]\n",
            "\n",
            "    82%|████████▏ | 98/120 [15:00<01:54,  5.21s/it]\n",
            "\n",
            "    82%|████████▎ | 99/120 [15:08<02:07,  6.06s/it]\n",
            "\n",
            "    83%|████████▎ | 100/120 [15:16<02:15,  6.75s/it]\n",
            "\n",
            "    84%|████████▍ | 101/120 [15:24<02:18,  7.28s/it]\n",
            "\n",
            "    85%|████████▌ | 102/120 [15:30<01:59,  6.64s/it]\n",
            "\n",
            "    86%|████████▌ | 103/120 [15:41<02:17,  8.06s/it]\n",
            "\n",
            "    87%|████████▋ | 104/120 [15:49<02:08,  8.01s/it]\n",
            "\n",
            "    88%|████████▊ | 105/120 [16:13<03:13, 12.89s/it]\n",
            "\n",
            "    88%|████████▊ | 106/120 [16:24<02:51, 12.23s/it]\n",
            "\n",
            "    89%|████████▉ | 107/120 [16:44<03:08, 14.52s/it]\n",
            "\n",
            "    90%|█████████ | 108/120 [16:56<02:47, 13.92s/it]\n",
            "\n",
            "    91%|█████████ | 109/120 [17:18<02:57, 16.17s/it]\n",
            "\n",
            "    92%|█████████▏| 110/120 [17:30<02:31, 15.15s/it]\n",
            "\n",
            "    92%|█████████▎| 111/120 [17:45<02:14, 14.90s/it]\n",
            "\n",
            "    93%|█████████▎| 112/120 [18:10<02:25, 18.14s/it]\n",
            "\n",
            "    94%|█████████▍| 113/120 [18:12<01:32, 13.24s/it]\n",
            "\n",
            "    95%|█████████▌| 114/120 [18:14<00:59,  9.88s/it]\n",
            "\n",
            "    96%|█████████▌| 115/120 [18:16<00:37,  7.50s/it]\n",
            "\n",
            "    97%|█████████▋| 116/120 [18:18<00:23,  5.82s/it]\n",
            "\n",
            "    98%|█████████▊| 117/120 [18:20<00:14,  4.68s/it]\n",
            "\n",
            "    98%|█████████▊| 118/120 [18:22<00:07,  3.84s/it]\n",
            "\n",
            "    99%|█████████▉| 119/120 [18:24<00:03,  3.27s/it]\n",
            "\n",
            "   100%|██████████| 120/120 [18:26<00:00,  2.84s/it]\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 120/120 [18:26<00:00,  2.84s/it]{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 7.151863291549191e-05, 'epoch': 1.0}\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "     7%|▋         | 2/30 [00:02<00:29,  1.04s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    10%|█         | 3/30 [00:02<00:22,  1.22it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    13%|█▎        | 4/30 [00:03<00:25,  1.03it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    17%|█▋        | 5/30 [00:04<00:24,  1.02it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    20%|██        | 6/30 [00:05<00:24,  1.02s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    23%|██▎       | 7/30 [00:06<00:22,  1.03it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    27%|██▋       | 8/30 [00:08<00:27,  1.25s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    30%|███       | 9/30 [00:09<00:26,  1.26s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    33%|███▎      | 10/30 [00:10<00:23,  1.17s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    37%|███▋      | 11/30 [00:11<00:21,  1.14s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    40%|████      | 12/30 [00:14<00:26,  1.45s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    43%|████▎     | 13/30 [00:16<00:30,  1.80s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    47%|████▋     | 14/30 [00:18<00:27,  1.70s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    50%|█████     | 15/30 [00:19<00:21,  1.46s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    53%|█████▎    | 16/30 [00:21<00:22,  1.61s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    57%|█████▋    | 17/30 [00:23<00:25,  1.96s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    60%|██████    | 18/30 [00:25<00:22,  1.86s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    63%|██████▎   | 19/30 [00:31<00:33,  3.05s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    67%|██████▋   | 20/30 [00:33<00:29,  2.92s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    70%|███████   | 21/30 [00:34<00:19,  2.16s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    73%|███████▎  | 22/30 [00:34<00:12,  1.60s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    77%|███████▋  | 23/30 [00:35<00:08,  1.25s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    80%|████████  | 24/30 [00:35<00:05,  1.02it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    83%|████████▎ | 25/30 [00:35<00:03,  1.28it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    87%|████████▋ | 26/30 [00:36<00:02,  1.50it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    90%|█████████ | 27/30 [00:36<00:01,  1.71it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    93%|█████████▎| 28/30 [00:36<00:01,  1.89it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    97%|█████████▋| 29/30 [00:37<00:00,  2.06it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 30/30 [00:37<00:00,  2.08it/s]\u001b[A\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "                                                  \n",
            "\n",
            "   \u001b[A\n",
            "\n",
            "   100%|██████████| 120/120 [19:06<00:00,  2.84s/it]\n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 30/30 [00:37<00:00,  2.08it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "                                                  \u001b[A/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-672c6f94-13d758da77a155e3270419c1;5916c096-1898-4041-912e-b391b4b10f14)\n",
            "\n",
            "   \n",
            "\n",
            "   Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
            "\n",
            "   Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 120/120 [19:08<00:00,  2.84s/it]\n",
            "\n",
            "   100%|██████████| 120/120 [19:08<00:00,  9.57s/it]\n",
            "\n",
            "   {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_ExactMatch': 0.0, 'eval_runtime': 39.9313, 'eval_samples_per_second': 0.751, 'eval_steps_per_second': 0.751, 'epoch': 1.0}\n",
            "\n",
            "   {'train_runtime': 1148.7891, 'train_samples_per_second': 0.522, 'train_steps_per_second': 0.104, 'train_loss': 0.0, 'epoch': 1.0}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 07:43:21,306] Trial 46 pruned. Trial pruned due to error: malformed node or string on line 1: <ast.Name object at 0x7d5a2295d480>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: malformed node or string on line 1: <ast.Name object at 0x7d5a2295d480>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=2 --learning_rate=0.00019010215069810362 --gradient_accumulation_steps=8 --torch_empty_cache_steps=2 --peft_r=664 --fp16=True --load_in_4bit=False --optim=adamw_torch --weight_decay=0.06531709929730932 --model_name=meta-llama/Llama-3.2-1B-Instruct --peft_lora_alpha=546 --fp16_opt_level=O3 --load_in_8bit=False\n",
            "   2024-11-07 07:43:26.796246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 07:43:26.830146: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 07:43:26.840699: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 07:43:26.862838: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 07:43:30.064820: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 70717442 || all params: 1306535940 || trainable%: 5.41\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:04,  6.68 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:03,  7.03 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 10.69 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:02, 11.57 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:02, 10.87 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 11.18 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01,  9.89 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.83 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.94 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.75 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:01,  7.77 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  5.95 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.94 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 11.31 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 15.87 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 15.36 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.39 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/60 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     2%|▏         | 1/60 [01:06<1:04:58, 66.07s/it]\n",
            "\n",
            "     3%|▎         | 2/60 [02:07<1:00:59, 63.10s/it]Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3579, in training_step\n",
            "\n",
            "       loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3633, in compute_loss\n",
            "\n",
            "       outputs = model(**inputs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 823, in forward\n",
            "\n",
            "       return model_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 811, in __call__\n",
            "\n",
            "       return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "\n",
            "       return func(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 2386, in forward\n",
            "\n",
            "       return self.base_model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n",
            "\n",
            "       return self.model.forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1385, in forward\n",
            "\n",
            "       outputs = self.transformer(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 915, in forward\n",
            "\n",
            "       causal_mask = self._update_causal_mask(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1024, in _update_causal_mask\n",
            "\n",
            "       causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1094, in _prepare_4d_causal_attention_mask_with_cache_position\n",
            "\n",
            "       causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.02 GiB. GPU 0 has a total capacity of 14.75 GiB of which 5.37 GiB is free. Process 413460 has 9.38 GiB memory in use. Of the allocated memory 8.87 GiB is allocated by PyTorch, and 381.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "     3%|▎         | 2/60 [02:43<1:19:07, 81.85s/it]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 07:46:39,496] Trial 47 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=2.1669383135847747e-05 --gradient_accumulation_steps=6 --torch_empty_cache_steps=5 --peft_r=664 --fp16=False --load_in_4bit=False --optim=adafactor --weight_decay=0.04496499875893943 --model_name=meta-llama/Llama-3.2-3B-Instruct --peft_lora_alpha=950 --load_in_8bit=True --llm_int8_has_fp16_weight=False --bnb_4bit_compute_dtype=bf16 --bnb_4bit_quant_type=nf4 --bnb_4bit_use_double_quant=False\n",
            "   2024-11-07 07:46:44.530992: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 07:46:44.551824: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 07:46:44.558633: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 07:46:44.575296: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 07:46:45.800629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   \n",
            "\n",
            "   Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "\n",
            "   Downloading shards:  50%|█████     | 1/2 [01:57<01:57, 117.90s/it]\n",
            "\n",
            "   Downloading shards: 100%|██████████| 2/2 [02:32<00:00, 68.77s/it] \n",
            "\n",
            "   Downloading shards: 100%|██████████| 2/2 [02:32<00:00, 76.14s/it]\n",
            "\n",
            "   Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 199, in <module>\n",
            "\n",
            "       model = AutoModelForQuestionAnswering.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "\n",
            "       return model_class.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4225, in from_pretrained\n",
            "\n",
            "       ) = cls._load_pretrained_model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4540, in _load_pretrained_model\n",
            "\n",
            "       model.apply(model._initialize_weights)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1029, in apply\n",
            "\n",
            "       module.apply(fn)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1029, in apply\n",
            "\n",
            "       module.apply(fn)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1029, in apply\n",
            "\n",
            "       module.apply(fn)\n",
            "\n",
            "     [Previous line repeated 2 more times]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1030, in apply\n",
            "\n",
            "       fn(self)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 1912, in _initialize_weights\n",
            "\n",
            "       self._init_weights(module)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 742, in _init_weights\n",
            "\n",
            "       module.weight.data.normal_(mean=0.0, std=std)\n",
            "\n",
            "   RuntimeError: \"normal_kernel_cpu\" not implemented for 'Char'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 07:49:49,736] Trial 48 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=2 --learning_rate=0.00012919730659628927 --gradient_accumulation_steps=10 --torch_empty_cache_steps=10 --peft_r=956 --fp16=True --load_in_4bit=True --optim=adamw_torch --weight_decay=0.01605717182578946 --model_name=Qwen/Qwen2.5-0.5B-Instruct --peft_lora_alpha=1023 --fp16_opt_level=O0 --llm_int8_has_fp16_weight=False --bnb_4bit_compute_dtype=bf16 --bnb_4bit_quant_type=fp4 --bnb_4bit_use_double_quant=True\n",
            "   2024-11-07 07:49:54.723933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 07:49:54.745301: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 07:49:54.751722: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 07:49:54.767295: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 07:49:55.927835: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 64612098 || all params: 379733380 || trainable%: 17.02\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:03,  8.05 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:03,  8.48 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 12.72 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:01, 13.85 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:01, 13.37 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 13.19 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01, 10.46 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.82 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.53 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.26 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:01<00:01,  7.31 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  5.70 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.61 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 11.07 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 16.12 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.63 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/60 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "\n",
            "   \n",
            "\n",
            "     2%|▏         | 1/60 [02:38<2:36:05, 158.74s/it]\n",
            "\n",
            "     3%|▎         | 2/60 [05:32<2:41:42, 167.29s/it]Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3579, in training_step\n",
            "\n",
            "       loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3633, in compute_loss\n",
            "\n",
            "       outputs = model(**inputs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 823, in forward\n",
            "\n",
            "       return model_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 811, in __call__\n",
            "\n",
            "       return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "\n",
            "       return func(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 2386, in forward\n",
            "\n",
            "       return self.base_model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n",
            "\n",
            "       return self.model.forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1463, in forward\n",
            "\n",
            "       outputs = self.model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 864, in forward\n",
            "\n",
            "       causal_mask = self._update_causal_mask(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 984, in _update_causal_mask\n",
            "\n",
            "       causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1070, in _prepare_4d_causal_attention_mask_with_cache_position\n",
            "\n",
            "       padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.36 GiB. GPU 0 has a total capacity of 14.75 GiB of which 5.04 GiB is free. Process 435495 has 9.71 GiB memory in use. Of the allocated memory 9.43 GiB is allocated by PyTorch, and 140.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "     3%|▎         | 2/60 [05:36<2:42:36, 168.22s/it]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 07:55:51,208] Trial 49 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=0.00010479357136906529 --gradient_accumulation_steps=6 --torch_empty_cache_steps=3 --peft_r=430 --fp16=True --load_in_4bit=True --optim=adafactor --weight_decay=0.028404534280674167 --model_name=meta-llama/Llama-3.2-3B-Instruct --peft_lora_alpha=128 --fp16_opt_level=O0 --llm_int8_has_fp16_weight=True --bnb_4bit_compute_dtype=bf16 --bnb_4bit_quant_type=fp4 --bnb_4bit_use_double_quant=False\n",
            "   2024-11-07 07:55:56.243481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 07:55:56.264203: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 07:55:56.270662: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 07:55:56.286672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 07:55:57.469390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   \n",
            "\n",
            "   Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "\n",
            "   Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.12it/s]\n",
            "\n",
            "   Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.58it/s]\n",
            "\n",
            "   Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n",
            "\n",
            "   Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 123295746 || all params: 1926765572 || trainable%: 6.40\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:04,  6.61 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:04,  6.97 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 10.21 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:02, 11.16 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:02, 10.86 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 11.24 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01,  9.96 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.66 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.72 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.83 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:01,  7.89 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  6.02 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.85 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 11.12 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 15.82 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 15.50 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.35 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [01:32<3:04:00, 92.77s/it]Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3612, in training_step\n",
            "\n",
            "       self.accelerator.backward(loss, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2237, in backward\n",
            "\n",
            "       self.scaler.scale(loss).backward(**kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n",
            "\n",
            "       torch.autograd.backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "\n",
            "       _engine_run_backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "\n",
            "       return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 307, in apply\n",
            "\n",
            "       return user_fn(self, *args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 321, in backward\n",
            "\n",
            "       torch.autograd.backward(outputs_with_grad, args_with_grad)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "\n",
            "       _engine_run_backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "\n",
            "       return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 161.06 MiB is free. Process 454813 has 14.59 GiB memory in use. Of the allocated memory 11.31 GiB is allocated by PyTorch, and 3.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [02:05<4:08:08, 125.11s/it]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 08:00:42,713] Trial 50 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=2 --learning_rate=7.80337544681098e-05 --gradient_accumulation_steps=2 --torch_empty_cache_steps=9 --peft_r=282 --fp16=True --load_in_4bit=True --optim=adamw_torch --weight_decay=0.033770941191442784 --model_name=meta-llama/Llama-3.2-3B-Instruct --peft_lora_alpha=985 --fp16_opt_level=O2 --llm_int8_has_fp16_weight=False --bnb_4bit_compute_dtype=fp32 --bnb_4bit_quant_type=fp4 --bnb_4bit_use_double_quant=True\n",
            "   2024-11-07 08:00:47.109196: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 08:00:47.130082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 08:00:47.136574: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 08:00:47.152412: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 08:00:48.318450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   \n",
            "\n",
            "   Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "\n",
            "   Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.91it/s]\n",
            "\n",
            "   Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.10it/s]\n",
            "\n",
            "   Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.07it/s]\n",
            "\n",
            "   Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 80861186 || all params: 1884331012 || trainable%: 4.29\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:04,  6.66 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:04,  6.99 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 10.69 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:02, 11.61 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:01, 11.00 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 11.43 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01, 10.06 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.87 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  9.00 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.93 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:01<00:01,  7.97 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  5.99 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.94 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 11.42 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 16.24 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 15.77 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.53 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/60 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3579, in training_step\n",
            "\n",
            "       loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3633, in compute_loss\n",
            "\n",
            "       outputs = model(**inputs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 823, in forward\n",
            "\n",
            "       return model_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 811, in __call__\n",
            "\n",
            "       return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "\n",
            "       return func(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 2386, in forward\n",
            "\n",
            "       return self.base_model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n",
            "\n",
            "       return self.model.forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1385, in forward\n",
            "\n",
            "       outputs = self.transformer(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 933, in forward\n",
            "\n",
            "       layer_outputs = self._gradient_checkpointing_func(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 32, in inner\n",
            "\n",
            "       return disable_fn(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\n",
            "\n",
            "       return fn(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 489, in checkpoint\n",
            "\n",
            "       return CheckpointFunction.apply(function, preserve, *args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 575, in apply\n",
            "\n",
            "       return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 264, in forward\n",
            "\n",
            "       outputs = run_function(*args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 692, in forward\n",
            "\n",
            "       hidden_states = self.mlp(hidden_states)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 258, in forward\n",
            "\n",
            "       down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 674.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 325.06 MiB is free. Process 470880 has 14.43 GiB memory in use. Of the allocated memory 11.13 GiB is allocated by PyTorch, and 3.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/60 [00:09<?, ?it/s]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 08:03:34,894] Trial 51 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=5.3578139592458247e-05 --gradient_accumulation_steps=9 --torch_empty_cache_steps=6 --peft_r=905 --fp16=False --load_in_4bit=False --optim=adafactor --weight_decay=0.07521918969055258 --model_name=meta-llama/Llama-3.2-1B-Instruct --peft_lora_alpha=527 --load_in_8bit=False\n",
            "   2024-11-07 08:03:38.770723: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 08:03:38.791580: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 08:03:38.798503: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 08:03:38.814836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 08:03:39.988210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 96382978 || all params: 1332201476 || trainable%: 7.23\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:04,  6.65 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:04,  6.97 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 10.66 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:02, 11.53 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:01, 11.17 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 11.33 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01,  9.77 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.73 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.87 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.99 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:01,  7.92 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  5.98 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.97 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 11.43 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 16.15 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 15.96 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.51 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [00:15<30:25, 15.34s/it]\n",
            "\n",
            "     2%|▏         | 2/120 [00:28<28:04, 14.28s/it]\n",
            "\n",
            "     2%|▎         | 3/120 [00:44<28:48, 14.77s/it]\n",
            "\n",
            "     3%|▎         | 4/120 [00:59<28:35, 14.79s/it]\n",
            "\n",
            "     4%|▍         | 5/120 [01:30<39:54, 20.82s/it]\n",
            "\n",
            "     5%|▌         | 6/120 [01:58<43:54, 23.11s/it]\n",
            "\n",
            "     6%|▌         | 7/120 [02:21<43:44, 23.23s/it]\n",
            "\n",
            "     7%|▋         | 8/120 [02:55<49:26, 26.48s/it]\n",
            "\n",
            "     8%|▊         | 9/120 [03:30<54:20, 29.37s/it]\n",
            "\n",
            "     8%|▊         | 10/120 [03:34<39:09, 21.36s/it]\n",
            "\n",
            "     9%|▉         | 11/120 [03:37<28:47, 15.85s/it]\n",
            "\n",
            "    10%|█         | 12/120 [03:41<21:51, 12.14s/it]\n",
            "\n",
            "    11%|█         | 13/120 [03:44<17:01,  9.55s/it]\n",
            "\n",
            "    12%|█▏        | 14/120 [03:54<17:01,  9.64s/it]\n",
            "\n",
            "    12%|█▎        | 15/120 [04:08<19:12, 10.98s/it]\n",
            "\n",
            "    13%|█▎        | 16/120 [04:24<21:35, 12.45s/it]\n",
            "\n",
            "    14%|█▍        | 17/120 [04:38<21:54, 12.76s/it]\n",
            "\n",
            "    15%|█▌        | 18/120 [05:01<27:11, 15.99s/it]\n",
            "\n",
            "    16%|█▌        | 19/120 [05:29<32:59, 19.60s/it]\n",
            "\n",
            "    17%|█▋        | 20/120 [05:58<37:26, 22.47s/it]\n",
            "\n",
            "    18%|█▊        | 21/120 [06:29<40:58, 24.84s/it]\n",
            "\n",
            "    18%|█▊        | 22/120 [06:59<43:13, 26.47s/it]\n",
            "\n",
            "    19%|█▉        | 23/120 [07:16<38:03, 23.54s/it]\n",
            "\n",
            "    20%|██        | 24/120 [07:19<28:00, 17.50s/it]\n",
            "\n",
            "    21%|██        | 25/120 [07:23<21:13, 13.41s/it]\n",
            "\n",
            "    22%|██▏       | 26/120 [07:27<16:26, 10.49s/it]\n",
            "\n",
            "    22%|██▎       | 27/120 [07:32<14:01,  9.05s/it]\n",
            "\n",
            "    23%|██▎       | 28/120 [07:47<16:31, 10.77s/it]\n",
            "\n",
            "    24%|██▍       | 29/120 [08:01<17:42, 11.67s/it]\n",
            "\n",
            "    25%|██▌       | 30/120 [08:14<18:02, 12.03s/it]\n",
            "\n",
            "    26%|██▌       | 31/120 [08:31<20:25, 13.77s/it]\n",
            "\n",
            "    27%|██▋       | 32/120 [09:04<28:27, 19.41s/it]\n",
            "\n",
            "    28%|██▊       | 33/120 [09:34<32:38, 22.51s/it]\n",
            "\n",
            "    28%|██▊       | 34/120 [10:06<36:17, 25.32s/it]\n",
            "\n",
            "    29%|██▉       | 35/120 [10:32<36:22, 25.68s/it]\n",
            "\n",
            "    30%|███       | 36/120 [10:59<36:32, 26.10s/it]\n",
            "\n",
            "    31%|███       | 37/120 [11:03<26:45, 19.35s/it]\n",
            "\n",
            "    32%|███▏      | 38/120 [11:06<19:56, 14.60s/it]\n",
            "\n",
            "    32%|███▎      | 39/120 [11:10<15:18, 11.34s/it]\n",
            "\n",
            "    33%|███▎      | 40/120 [11:13<11:56,  8.95s/it]\n",
            "\n",
            "    34%|███▍      | 41/120 [11:27<13:37, 10.35s/it]\n",
            "\n",
            "    35%|███▌      | 42/120 [11:41<14:43, 11.32s/it]\n",
            "\n",
            "    36%|███▌      | 43/120 [11:56<16:06, 12.56s/it]\n",
            "\n",
            "    37%|███▋      | 44/120 [12:11<16:41, 13.17s/it]\n",
            "\n",
            "    38%|███▊      | 45/120 [12:42<23:21, 18.68s/it]\n",
            "\n",
            "    38%|███▊      | 46/120 [13:10<26:21, 21.38s/it]\n",
            "\n",
            "    39%|███▉      | 47/120 [13:33<26:40, 21.93s/it]\n",
            "\n",
            "    40%|████      | 48/120 [14:07<30:25, 25.35s/it]\n",
            "\n",
            "    41%|████      | 49/120 [14:43<33:58, 28.71s/it]\n",
            "\n",
            "    42%|████▏     | 50/120 [14:46<24:38, 21.12s/it]\n",
            "\n",
            "    42%|████▎     | 51/120 [14:50<18:09, 15.79s/it]\n",
            "\n",
            "    43%|████▎     | 52/120 [14:53<13:46, 12.15s/it]\n",
            "\n",
            "    44%|████▍     | 53/120 [14:57<10:39,  9.55s/it]\n",
            "\n",
            "    45%|████▌     | 54/120 [15:07<10:35,  9.63s/it]\n",
            "\n",
            "    46%|████▌     | 55/120 [15:21<12:02, 11.11s/it]\n",
            "\n",
            "    47%|████▋     | 56/120 [15:37<13:24, 12.57s/it]\n",
            "\n",
            "    48%|████▊     | 57/120 [15:51<13:31, 12.88s/it]\n",
            "\n",
            "    48%|████▊     | 58/120 [16:15<16:39, 16.12s/it]\n",
            "\n",
            "    49%|████▉     | 59/120 [16:43<19:59, 19.67s/it]\n",
            "\n",
            "    50%|█████     | 60/120 [17:12<22:31, 22.52s/it]\n",
            "\n",
            "    51%|█████     | 61/120 [17:42<24:29, 24.91s/it]\n",
            "\n",
            "    52%|█████▏    | 62/120 [18:12<25:37, 26.51s/it]\n",
            "\n",
            "    52%|█████▎    | 63/120 [18:29<22:23, 23.57s/it]\n",
            "\n",
            "    53%|█████▎    | 64/120 [18:33<16:21, 17.53s/it]\n",
            "\n",
            "    54%|█████▍    | 65/120 [18:36<12:15, 13.37s/it]\n",
            "\n",
            "    55%|█████▌    | 66/120 [18:40<09:24, 10.46s/it]\n",
            "\n",
            "    56%|█████▌    | 67/120 [18:46<08:00,  9.07s/it]\n",
            "\n",
            "    57%|█████▋    | 68/120 [19:01<09:21, 10.80s/it]\n",
            "\n",
            "    57%|█████▊    | 69/120 [19:14<09:55, 11.68s/it]\n",
            "\n",
            "    58%|█████▊    | 70/120 [19:27<10:01, 12.04s/it]\n",
            "\n",
            "    59%|█████▉    | 71/120 [19:45<11:12, 13.73s/it]\n",
            "\n",
            "    60%|██████    | 72/120 [20:17<15:29, 19.36s/it]\n",
            "\n",
            "    61%|██████    | 73/120 [20:47<17:29, 22.32s/it]\n",
            "\n",
            "    62%|██████▏   | 74/120 [21:18<19:08, 24.97s/it]\n",
            "\n",
            "    62%|██████▎   | 75/120 [21:44<18:57, 25.28s/it]\n",
            "\n",
            "    63%|██████▎   | 76/120 [22:11<18:57, 25.85s/it]\n",
            "\n",
            "    64%|██████▍   | 77/120 [22:14<13:42, 19.14s/it]\n",
            "\n",
            "    65%|██████▌   | 78/120 [22:18<10:06, 14.45s/it]\n",
            "\n",
            "    66%|██████▌   | 79/120 [22:22<07:40, 11.23s/it]\n",
            "\n",
            "    67%|██████▋   | 80/120 [22:25<05:57,  8.93s/it]\n",
            "\n",
            "    68%|██████▊   | 81/120 [22:39<06:43, 10.34s/it]\n",
            "\n",
            "    68%|██████▊   | 82/120 [22:53<07:14, 11.45s/it]\n",
            "\n",
            "    69%|██████▉   | 83/120 [23:09<07:50, 12.71s/it]\n",
            "\n",
            "    70%|███████   | 84/120 [23:24<08:03, 13.43s/it]\n",
            "\n",
            "    71%|███████   | 85/120 [23:56<11:05, 19.00s/it]\n",
            "\n",
            "    72%|███████▏  | 86/120 [24:23<12:15, 21.63s/it]\n",
            "\n",
            "    72%|███████▎  | 87/120 [24:47<12:13, 22.24s/it]\n",
            "\n",
            "    73%|███████▎  | 88/120 [25:20<13:31, 25.34s/it]\n",
            "\n",
            "    74%|███████▍  | 89/120 [25:56<14:49, 28.71s/it]\n",
            "\n",
            "    75%|███████▌  | 90/120 [26:00<10:33, 21.12s/it]\n",
            "\n",
            "    76%|███████▌  | 91/120 [26:03<07:38, 15.83s/it]\n",
            "\n",
            "    77%|███████▋  | 92/120 [26:07<05:40, 12.15s/it]\n",
            "\n",
            "    78%|███████▊  | 93/120 [26:10<04:18,  9.59s/it]\n",
            "\n",
            "    78%|███████▊  | 94/120 [26:20<04:11,  9.66s/it]\n",
            "\n",
            "    79%|███████▉  | 95/120 [26:34<04:35, 11.01s/it]\n",
            "\n",
            "    80%|████████  | 96/120 [26:50<04:56, 12.36s/it]\n",
            "\n",
            "    81%|████████  | 97/120 [27:03<04:52, 12.72s/it]\n",
            "\n",
            "    82%|████████▏ | 98/120 [27:26<05:48, 15.84s/it]\n",
            "\n",
            "    82%|████████▎ | 99/120 [27:54<06:48, 19.47s/it]\n",
            "\n",
            "    83%|████████▎ | 100/120 [28:24<07:31, 22.56s/it]\n",
            "\n",
            "    84%|████████▍ | 101/120 [28:55<07:56, 25.09s/it]\n",
            "\n",
            "    85%|████████▌ | 102/120 [29:26<08:01, 26.77s/it]\n",
            "\n",
            "    86%|████████▌ | 103/120 [29:43<06:45, 23.85s/it]\n",
            "\n",
            "    87%|████████▋ | 104/120 [29:46<04:43, 17.72s/it]\n",
            "\n",
            "    88%|████████▊ | 105/120 [29:50<03:22, 13.49s/it]\n",
            "\n",
            "    88%|████████▊ | 106/120 [29:53<02:27, 10.51s/it]\n",
            "\n",
            "    89%|████████▉ | 107/120 [29:59<01:59,  9.16s/it]\n",
            "\n",
            "    90%|█████████ | 108/120 [30:14<02:10, 10.86s/it]\n",
            "\n",
            "    91%|█████████ | 109/120 [30:28<02:09, 11.77s/it]\n",
            "\n",
            "    92%|█████████▏| 110/120 [30:41<02:01, 12.11s/it]\n",
            "\n",
            "    92%|█████████▎| 111/120 [30:59<02:04, 13.78s/it]\n",
            "\n",
            "    93%|█████████▎| 112/120 [31:31<02:35, 19.41s/it]\n",
            "\n",
            "    94%|█████████▍| 113/120 [32:00<02:35, 22.28s/it]\n",
            "\n",
            "    95%|█████████▌| 114/120 [32:31<02:29, 24.92s/it]\n",
            "\n",
            "    96%|█████████▌| 115/120 [32:58<02:06, 25.28s/it]\n",
            "\n",
            "    97%|█████████▋| 116/120 [33:25<01:43, 25.87s/it]\n",
            "\n",
            "    98%|█████████▊| 117/120 [33:28<00:57, 19.16s/it]\n",
            "\n",
            "    98%|█████████▊| 118/120 [33:32<00:28, 14.46s/it]\n",
            "\n",
            "    99%|█████████▉| 119/120 [33:35<00:11, 11.20s/it]\n",
            "\n",
            "   100%|██████████| 120/120 [33:39<00:00,  8.92s/it]\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 120/120 [33:39<00:00,  8.92s/it]{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 5.3578139592458247e-05, 'epoch': 1.0}\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   \n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "     7%|▋         | 2/30 [00:02<00:29,  1.04s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    10%|█         | 3/30 [00:02<00:22,  1.22it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    13%|█▎        | 4/30 [00:03<00:25,  1.03it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    17%|█▋        | 5/30 [00:04<00:24,  1.02it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    20%|██        | 6/30 [00:05<00:24,  1.02s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    23%|██▎       | 7/30 [00:06<00:22,  1.03it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    27%|██▋       | 8/30 [00:08<00:27,  1.25s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    30%|███       | 9/30 [00:09<00:26,  1.26s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    33%|███▎      | 10/30 [00:10<00:23,  1.17s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    37%|███▋      | 11/30 [00:11<00:21,  1.14s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    40%|████      | 12/30 [00:14<00:26,  1.45s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    43%|████▎     | 13/30 [00:16<00:30,  1.80s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    47%|████▋     | 14/30 [00:18<00:27,  1.70s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    50%|█████     | 15/30 [00:19<00:21,  1.46s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    53%|█████▎    | 16/30 [00:21<00:22,  1.61s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    57%|█████▋    | 17/30 [00:23<00:25,  1.96s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    60%|██████    | 18/30 [00:25<00:22,  1.85s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    63%|██████▎   | 19/30 [00:31<00:33,  3.04s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    67%|██████▋   | 20/30 [00:33<00:29,  2.91s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    70%|███████   | 21/30 [00:34<00:19,  2.15s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    73%|███████▎  | 22/30 [00:34<00:12,  1.60s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    77%|███████▋  | 23/30 [00:34<00:08,  1.24s/it]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    80%|████████  | 24/30 [00:35<00:05,  1.03it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    83%|████████▎ | 25/30 [00:35<00:03,  1.29it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    87%|████████▋ | 26/30 [00:36<00:02,  1.52it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    90%|█████████ | 27/30 [00:36<00:01,  1.73it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    93%|█████████▎| 28/30 [00:36<00:01,  1.91it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "    97%|█████████▋| 29/30 [00:37<00:00,  2.08it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 30/30 [00:37<00:00,  2.15it/s]\u001b[A\n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "                                                  \n",
            "\n",
            "   \u001b[A\n",
            "\n",
            "   100%|██████████| 120/120 [34:19<00:00,  8.92s/it]\n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 30/30 [00:37<00:00,  2.15it/s]\u001b[A\n",
            "\n",
            "   \n",
            "\n",
            "                                                  \u001b[A/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-672c7c80-651cd73b65ef935d7bd5c4de;bd6a467e-0ca5-4770-9d8c-157228bd1579)\n",
            "\n",
            "   \n",
            "\n",
            "   Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct/resolve/main/config.json.\n",
            "\n",
            "   Access to model meta-llama/Llama-3.2-1B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in meta-llama/Llama-3.2-1B-Instruct.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in meta-llama/Llama-3.2-1B-Instruct - will assume that the vocabulary was not modified.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "                                                    \n",
            "\n",
            "   \n",
            "\n",
            "   100%|██████████| 120/120 [34:22<00:00,  8.92s/it]\n",
            "\n",
            "   100%|██████████| 120/120 [34:22<00:00, 17.19s/it]\n",
            "\n",
            "   {'eval_loss': nan, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_ExactMatch': 0.0, 'eval_runtime': 39.8389, 'eval_samples_per_second': 0.753, 'eval_steps_per_second': 0.753, 'epoch': 1.0}\n",
            "\n",
            "   {'train_runtime': 2062.4897, 'train_samples_per_second': 0.524, 'train_steps_per_second': 0.058, 'train_loss': 0.0, 'epoch': 1.0}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 08:38:30,095] Trial 52 pruned. Trial pruned due to error: malformed node or string on line 1: <ast.Name object at 0x7d5a2295c250>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: malformed node or string on line 1: <ast.Name object at 0x7d5a2295c250>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=2 --learning_rate=0.0001783044424359261 --gradient_accumulation_steps=8 --torch_empty_cache_steps=1 --peft_r=402 --fp16=False --load_in_4bit=False --optim=adamw_bnb_8bit --weight_decay=0.0682514295802492 --model_name=Qwen/Qwen2.5-0.5B-Instruct --peft_lora_alpha=962 --load_in_8bit=True --llm_int8_has_fp16_weight=True --bnb_4bit_compute_dtype=fp32 --bnb_4bit_quant_type=nf4 --bnb_4bit_use_double_quant=True\n",
            "   2024-11-07 08:38:36.550575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 08:38:36.571724: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 08:38:36.578251: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 08:38:36.594715: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 08:38:37.765615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 199, in <module>\n",
            "\n",
            "       model = AutoModelForQuestionAnswering.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "\n",
            "       return model_class.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4303, in from_pretrained\n",
            "\n",
            "       dispatch_model(model, **device_map_kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py\", line 352, in dispatch_model\n",
            "\n",
            "       check_device_map(model, device_map)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\", line 1407, in check_device_map\n",
            "\n",
            "       all_model_tensors = [name for name, _ in model.state_dict().items()]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2219, in state_dict\n",
            "\n",
            "       module.state_dict(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2219, in state_dict\n",
            "\n",
            "       module.state_dict(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2219, in state_dict\n",
            "\n",
            "       module.state_dict(\n",
            "\n",
            "     [Previous line repeated 2 more times]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2216, in state_dict\n",
            "\n",
            "       self._save_to_state_dict(destination, prefix, keep_vars)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\", line 928, in _save_to_state_dict\n",
            "\n",
            "       param_from_weight = getattr(self.weight, scb_name)\n",
            "\n",
            "   AttributeError: 'Tensor' object has no attribute 'SCB'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 08:38:47,508] Trial 53 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=0.00027684889764075145 --gradient_accumulation_steps=8 --torch_empty_cache_steps=3 --peft_r=895 --fp16=True --load_in_4bit=True --optim=adamw_torch --weight_decay=0.09223820179520058 --model_name=meta-llama/Llama-3.2-3B-Instruct --peft_lora_alpha=515 --fp16_opt_level=O2 --llm_int8_has_fp16_weight=True --bnb_4bit_compute_dtype=fp32 --bnb_4bit_quant_type=fp4 --bnb_4bit_use_double_quant=False\n",
            "   2024-11-07 08:38:51.462240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 08:38:51.482813: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 08:38:51.489449: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 08:38:51.506075: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 08:38:52.678030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   \n",
            "\n",
            "   Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "\n",
            "   Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.13it/s]\n",
            "\n",
            "   Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.38it/s]\n",
            "\n",
            "   Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.34it/s]\n",
            "\n",
            "   Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 256620546 || all params: 2060090372 || trainable%: 12.46\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:05,  5.74 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:04,  6.59 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 10.41 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:02, 11.41 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:02, 10.96 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 11.16 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01,  9.88 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.83 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.97 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.92 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:01,  7.94 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  6.04 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  6.05 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 11.55 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 16.49 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.49 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [02:03<4:04:58, 123.52s/it]Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3579, in training_step\n",
            "\n",
            "       loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3633, in compute_loss\n",
            "\n",
            "       outputs = model(**inputs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 823, in forward\n",
            "\n",
            "       return model_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 811, in __call__\n",
            "\n",
            "       return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "\n",
            "       return func(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 2386, in forward\n",
            "\n",
            "       return self.base_model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n",
            "\n",
            "       return self.model.forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1385, in forward\n",
            "\n",
            "       outputs = self.transformer(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 933, in forward\n",
            "\n",
            "       layer_outputs = self._gradient_checkpointing_func(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 32, in inner\n",
            "\n",
            "       return disable_fn(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\n",
            "\n",
            "       return fn(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 489, in checkpoint\n",
            "\n",
            "       return CheckpointFunction.apply(function, preserve, *args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 575, in apply\n",
            "\n",
            "       return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 264, in forward\n",
            "\n",
            "       outputs = run_function(*args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 692, in forward\n",
            "\n",
            "       hidden_states = self.mlp(hidden_states)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 258, in forward\n",
            "\n",
            "       down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 468.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 309.06 MiB is free. Process 590039 has 14.44 GiB memory in use. Of the allocated memory 10.51 GiB is allocated by PyTorch, and 3.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [02:50<5:37:42, 170.27s/it]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 08:44:22,871] Trial 54 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=0.00022486148839054228 --gradient_accumulation_steps=10 --torch_empty_cache_steps=2 --peft_r=970 --fp16=False --load_in_4bit=True --optim=adafactor --weight_decay=0.06697678095155545 --model_name=Qwen/Qwen2.5-1.5B-Instruct --peft_lora_alpha=117 --llm_int8_has_fp16_weight=False --bnb_4bit_compute_dtype=fp32 --bnb_4bit_quant_type=fp4 --bnb_4bit_use_double_quant=True\n",
            "   2024-11-07 08:44:27.114098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 08:44:27.136545: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 08:44:27.143590: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 08:44:27.159179: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 08:44:28.592001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 132109314 || all params: 1020728836 || trainable%: 12.94\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:03,  7.67 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:03,  8.13 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 12.28 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:01, 13.34 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:01, 12.93 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 13.06 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01, 10.39 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.63 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.40 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.24 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:01<00:01,  7.33 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  5.61 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.49 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 10.79 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 15.78 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.46 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [01:21<2:41:37, 81.49s/it]\n",
            "\n",
            "     2%|▏         | 2/120 [02:49<2:47:35, 85.22s/it]\n",
            "\n",
            "     2%|▎         | 3/120 [04:03<2:36:47, 80.40s/it]\n",
            "\n",
            "     3%|▎         | 4/120 [05:46<2:51:58, 88.95s/it]Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3612, in training_step\n",
            "\n",
            "       self.accelerator.backward(loss, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2237, in backward\n",
            "\n",
            "       self.scaler.scale(loss).backward(**kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n",
            "\n",
            "       torch.autograd.backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "\n",
            "       _engine_run_backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "\n",
            "       return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 307, in apply\n",
            "\n",
            "       return user_fn(self, *args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 321, in backward\n",
            "\n",
            "       torch.autograd.backward(outputs_with_grad, args_with_grad)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "\n",
            "       _engine_run_backward(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "\n",
            "       return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.05 GiB. GPU 0 has a total capacity of 14.75 GiB of which 577.06 MiB is free. Process 607882 has 14.18 GiB memory in use. Of the allocated memory 13.62 GiB is allocated by PyTorch, and 434.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "     3%|▎         | 4/120 [06:06<2:56:55, 91.51s/it]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 08:51:03,928] Trial 55 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=2 --learning_rate=8.087667227445726e-05 --gradient_accumulation_steps=2 --torch_empty_cache_steps=9 --peft_r=813 --fp16=True --load_in_4bit=True --optim=adamw_torch --weight_decay=0.05366442238534025 --model_name=meta-llama/Llama-3.2-3B-Instruct --peft_lora_alpha=88 --fp16_opt_level=O2 --llm_int8_has_fp16_weight=True --bnb_4bit_compute_dtype=bf16 --bnb_4bit_quant_type=nf4 --bnb_4bit_use_double_quant=False\n",
            "   2024-11-07 08:51:08.332252: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 08:51:08.352909: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 08:51:08.359372: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 08:51:08.375018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 08:51:09.555517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   \n",
            "\n",
            "   Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "\n",
            "   Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.44it/s]\n",
            "\n",
            "   Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.48it/s]\n",
            "\n",
            "   Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.47it/s]\n",
            "\n",
            "   Some weights of LlamaForQuestionAnswering were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.18.input_layernorm.weight', 'layers.18.mlp.down_proj.weight', 'layers.18.mlp.gate_proj.weight', 'layers.18.mlp.up_proj.weight', 'layers.18.post_attention_layernorm.weight', 'layers.18.self_attn.k_proj.weight', 'layers.18.self_attn.o_proj.weight', 'layers.18.self_attn.q_proj.weight', 'layers.18.self_attn.v_proj.weight', 'layers.19.input_layernorm.weight', 'layers.19.mlp.down_proj.weight', 'layers.19.mlp.gate_proj.weight', 'layers.19.mlp.up_proj.weight', 'layers.19.post_attention_layernorm.weight', 'layers.19.self_attn.k_proj.weight', 'layers.19.self_attn.o_proj.weight', 'layers.19.self_attn.q_proj.weight', 'layers.19.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.20.input_layernorm.weight', 'layers.20.mlp.down_proj.weight', 'layers.20.mlp.gate_proj.weight', 'layers.20.mlp.up_proj.weight', 'layers.20.post_attention_layernorm.weight', 'layers.20.self_attn.k_proj.weight', 'layers.20.self_attn.o_proj.weight', 'layers.20.self_attn.q_proj.weight', 'layers.20.self_attn.v_proj.weight', 'layers.21.input_layernorm.weight', 'layers.21.mlp.down_proj.weight', 'layers.21.mlp.gate_proj.weight', 'layers.21.mlp.up_proj.weight', 'layers.21.post_attention_layernorm.weight', 'layers.21.self_attn.k_proj.weight', 'layers.21.self_attn.o_proj.weight', 'layers.21.self_attn.q_proj.weight', 'layers.21.self_attn.v_proj.weight', 'layers.22.input_layernorm.weight', 'layers.22.mlp.down_proj.weight', 'layers.22.mlp.gate_proj.weight', 'layers.22.mlp.up_proj.weight', 'layers.22.post_attention_layernorm.weight', 'layers.22.self_attn.k_proj.weight', 'layers.22.self_attn.o_proj.weight', 'layers.22.self_attn.q_proj.weight', 'layers.22.self_attn.v_proj.weight', 'layers.23.input_layernorm.weight', 'layers.23.mlp.down_proj.weight', 'layers.23.mlp.gate_proj.weight', 'layers.23.mlp.up_proj.weight', 'layers.23.post_attention_layernorm.weight', 'layers.23.self_attn.k_proj.weight', 'layers.23.self_attn.o_proj.weight', 'layers.23.self_attn.q_proj.weight', 'layers.23.self_attn.v_proj.weight', 'layers.24.input_layernorm.weight', 'layers.24.mlp.down_proj.weight', 'layers.24.mlp.gate_proj.weight', 'layers.24.mlp.up_proj.weight', 'layers.24.post_attention_layernorm.weight', 'layers.24.self_attn.k_proj.weight', 'layers.24.self_attn.o_proj.weight', 'layers.24.self_attn.q_proj.weight', 'layers.24.self_attn.v_proj.weight', 'layers.25.input_layernorm.weight', 'layers.25.mlp.down_proj.weight', 'layers.25.mlp.gate_proj.weight', 'layers.25.mlp.up_proj.weight', 'layers.25.post_attention_layernorm.weight', 'layers.25.self_attn.k_proj.weight', 'layers.25.self_attn.o_proj.weight', 'layers.25.self_attn.q_proj.weight', 'layers.25.self_attn.v_proj.weight', 'layers.26.input_layernorm.weight', 'layers.26.mlp.down_proj.weight', 'layers.26.mlp.gate_proj.weight', 'layers.26.mlp.up_proj.weight', 'layers.26.post_attention_layernorm.weight', 'layers.26.self_attn.k_proj.weight', 'layers.26.self_attn.o_proj.weight', 'layers.26.self_attn.q_proj.weight', 'layers.26.self_attn.v_proj.weight', 'layers.27.input_layernorm.weight', 'layers.27.mlp.down_proj.weight', 'layers.27.mlp.gate_proj.weight', 'layers.27.mlp.up_proj.weight', 'layers.27.post_attention_layernorm.weight', 'layers.27.self_attn.k_proj.weight', 'layers.27.self_attn.o_proj.weight', 'layers.27.self_attn.q_proj.weight', 'layers.27.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight', 'qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 233109506 || all params: 2036579332 || trainable%: 11.45\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:04,  6.61 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:04,  6.99 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 10.16 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:02, 11.16 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:02, 10.84 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 11.19 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01, 10.01 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.74 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.84 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.94 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:01,  7.92 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  6.07 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.97 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 11.36 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 16.08 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 15.93 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.47 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/60 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "\n",
            "   Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3579, in training_step\n",
            "\n",
            "       loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3633, in compute_loss\n",
            "\n",
            "       outputs = model(**inputs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 823, in forward\n",
            "\n",
            "       return model_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 811, in __call__\n",
            "\n",
            "       return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "\n",
            "       return func(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 2386, in forward\n",
            "\n",
            "       return self.base_model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n",
            "\n",
            "       return self.model.forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 1385, in forward\n",
            "\n",
            "       outputs = self.transformer(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 933, in forward\n",
            "\n",
            "       layer_outputs = self._gradient_checkpointing_func(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_compile.py\", line 32, in inner\n",
            "\n",
            "       return disable_fn(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\n",
            "\n",
            "       return fn(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 489, in checkpoint\n",
            "\n",
            "       return CheckpointFunction.apply(function, preserve, *args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 575, in apply\n",
            "\n",
            "       return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\", line 264, in forward\n",
            "\n",
            "       outputs = run_function(*args)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 692, in forward\n",
            "\n",
            "       hidden_states = self.mlp(hidden_states)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 258, in forward\n",
            "\n",
            "       down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 674.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 483.06 MiB is free. Process 628881 has 14.27 GiB memory in use. Of the allocated memory 11.52 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/60 [00:08<?, ?it/s]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 08:53:56,945] Trial 56 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=2 --learning_rate=0.00016334676179313718 --gradient_accumulation_steps=8 --torch_empty_cache_steps=10 --peft_r=909 --fp16=True --load_in_4bit=True --optim=adamw_bnb_8bit --weight_decay=0.026574642123632863 --model_name=Qwen/Qwen2.5-0.5B-Instruct --peft_lora_alpha=158 --fp16_opt_level=O2 --llm_int8_has_fp16_weight=False --bnb_4bit_compute_dtype=fp32 --bnb_4bit_quant_type=fp4 --bnb_4bit_use_double_quant=True\n",
            "   2024-11-07 08:54:01.410065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 08:54:01.430912: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 08:54:01.437327: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 08:54:01.452993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 08:54:02.660282: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 61435650 || all params: 376556932 || trainable%: 16.32\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:03,  7.81 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:03,  8.20 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 12.43 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:01, 13.16 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:01, 12.86 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 13.07 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01, 10.28 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.52 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.29 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.23 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:01,  7.23 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  5.51 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.50 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 10.68 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 15.73 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.43 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/60 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     2%|▏         | 1/60 [02:12<2:10:36, 132.83s/it]\n",
            "\n",
            "     3%|▎         | 2/60 [04:26<2:08:51, 133.29s/it]Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 291, in <module>\n",
            "\n",
            "       trainer.train()\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2123, in train\n",
            "\n",
            "       return inner_training_loop(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2481, in _inner_training_loop\n",
            "\n",
            "       tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3579, in training_step\n",
            "\n",
            "       loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3633, in compute_loss\n",
            "\n",
            "       outputs = model(**inputs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 823, in forward\n",
            "\n",
            "       return model_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 811, in __call__\n",
            "\n",
            "       return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 44, in decorate_autocast\n",
            "\n",
            "       return func(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 2386, in forward\n",
            "\n",
            "       return self.base_model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 197, in forward\n",
            "\n",
            "       return self.model.forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1463, in forward\n",
            "\n",
            "       outputs = self.model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "\n",
            "       return self._call_impl(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "\n",
            "       return forward_call(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "\n",
            "       output = module._old_forward(*args, **kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 864, in forward\n",
            "\n",
            "       causal_mask = self._update_causal_mask(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 984, in _update_causal_mask\n",
            "\n",
            "       causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1066, in _prepare_4d_causal_attention_mask_with_cache_position\n",
            "\n",
            "       causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n",
            "\n",
            "   torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.36 GiB. GPU 0 has a total capacity of 14.75 GiB of which 7.07 GiB is free. Process 638014 has 7.68 GiB memory in use. Of the allocated memory 5.97 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "   \n",
            "\n",
            "     3%|▎         | 2/60 [05:42<2:45:41, 171.41s/it]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 08:59:59,262] Trial 57 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=2 --learning_rate=0.00014233459806555114 --gradient_accumulation_steps=2 --torch_empty_cache_steps=4 --peft_r=402 --fp16=True --load_in_4bit=False --optim=adafactor --weight_decay=0.037384954043984876 --model_name=Qwen/Qwen2.5-0.5B-Instruct --peft_lora_alpha=828 --fp16_opt_level=O1 --load_in_8bit=True --llm_int8_has_fp16_weight=True --bnb_4bit_compute_dtype=fp32 --bnb_4bit_quant_type=nf4 --bnb_4bit_use_double_quant=False\n",
            "   2024-11-07 09:00:03.353272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 09:00:03.374577: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 09:00:03.381422: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 09:00:03.397411: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 09:00:04.569369: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 199, in <module>\n",
            "\n",
            "       model = AutoModelForQuestionAnswering.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "\n",
            "       return model_class.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4303, in from_pretrained\n",
            "\n",
            "       dispatch_model(model, **device_map_kwargs)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py\", line 352, in dispatch_model\n",
            "\n",
            "       check_device_map(model, device_map)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\", line 1407, in check_device_map\n",
            "\n",
            "       all_model_tensors = [name for name, _ in model.state_dict().items()]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2219, in state_dict\n",
            "\n",
            "       module.state_dict(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2219, in state_dict\n",
            "\n",
            "       module.state_dict(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2219, in state_dict\n",
            "\n",
            "       module.state_dict(\n",
            "\n",
            "     [Previous line repeated 2 more times]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 2216, in state_dict\n",
            "\n",
            "       self._save_to_state_dict(destination, prefix, keep_vars)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\", line 928, in _save_to_state_dict\n",
            "\n",
            "       param_from_weight = getattr(self.weight, scb_name)\n",
            "\n",
            "   AttributeError: 'Tensor' object has no attribute 'SCB'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 09:00:12,736] Trial 58 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=0.0002914201318358158 --gradient_accumulation_steps=1 --torch_empty_cache_steps=8 --peft_r=880 --fp16=False --load_in_4bit=False --optim=adafactor --weight_decay=0.024058085411881668 --model_name=meta-llama/Llama-3.2-1B-Instruct --peft_lora_alpha=402 --load_in_8bit=True --llm_int8_has_fp16_weight=False --bnb_4bit_compute_dtype=fp32 --bnb_4bit_quant_type=fp4 --bnb_4bit_use_double_quant=True\n",
            "   2024-11-07 09:00:16.732380: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 09:00:16.752777: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 09:00:16.759118: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 09:00:16.773995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 09:00:17.933116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Traceback (most recent call last):\n",
            "\n",
            "     File \"/content/drive/MyDrive/models/training_simpleqa.py\", line 199, in <module>\n",
            "\n",
            "       model = AutoModelForQuestionAnswering.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "\n",
            "       return model_class.from_pretrained(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4225, in from_pretrained\n",
            "\n",
            "       ) = cls._load_pretrained_model(\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 4540, in _load_pretrained_model\n",
            "\n",
            "       model.apply(model._initialize_weights)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1029, in apply\n",
            "\n",
            "       module.apply(fn)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1029, in apply\n",
            "\n",
            "       module.apply(fn)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1029, in apply\n",
            "\n",
            "       module.apply(fn)\n",
            "\n",
            "     [Previous line repeated 2 more times]\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1030, in apply\n",
            "\n",
            "       fn(self)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 1912, in _initialize_weights\n",
            "\n",
            "       self._init_weights(module)\n",
            "\n",
            "     File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\", line 742, in _init_weights\n",
            "\n",
            "       module.weight.data.normal_(mean=0.0, std=std)\n",
            "\n",
            "   RuntimeError: \"normal_kernel_cpu\" not implemented for 'Char'\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-11-07 09:00:37,858] Trial 59 pruned. Trial pruned due to error: no eval_loss\n",
            "/usr/local/lib/python3.10/dist-packages/optuna/study/_optimize.py:186: ExperimentalWarning: fail_stale_trials is experimental (supported from v2.9.0). The interface can change in the future.\n",
            "  optuna.storages.fail_stale_trials(study)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "command is: python /content/drive/MyDrive/models/training_simpleqa.py --batch_size=1 --learning_rate=0.00010131281787056267 --gradient_accumulation_steps=4 --torch_empty_cache_steps=9 --peft_r=283 --fp16=True --load_in_4bit=True --optim=adamw_torch --weight_decay=0.09585633640576324 --model_name=Qwen/Qwen2.5-0.5B-Instruct --peft_lora_alpha=408 --fp16_opt_level=O2 --llm_int8_has_fp16_weight=False --bnb_4bit_compute_dtype=fp32 --bnb_4bit_quant_type=fp4 --bnb_4bit_use_double_quant=False\n",
            "   2024-11-07 09:00:41.889752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "   2024-11-07 09:00:41.914759: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\n",
            "   2024-11-07 09:00:41.921228: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "   2024-11-07 09:00:41.936385: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "\n",
            "   To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "   2024-11-07 09:00:43.089113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\n",
            "   Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "   \n",
            "\n",
            "   You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\n",
            "   This will raise in a future version.\n",
            "\n",
            "   \n",
            "\n",
            "     warnings.warn(msg, FutureWarning)\n",
            "\n",
            "   Some weights of Qwen2ForQuestionAnswering were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "\n",
            "   You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "   trainable params: 19128066 || all params: 334249348 || trainable%: 5.72\n",
            "\n",
            "   Fold 0:\n",
            "\n",
            "     Train: 120 examples\n",
            "\n",
            "     Test: 30 examples\n",
            "\n",
            "   \n",
            "\n",
            "   Map:   0%|          | 0/30 [00:00<?, ? examples/s]\n",
            "\n",
            "   Map:   3%|▎         | 1/30 [00:00<00:04,  6.28 examples/s]\n",
            "\n",
            "   Map:   7%|▋         | 2/30 [00:00<00:03,  7.42 examples/s]\n",
            "\n",
            "   Map:  13%|█▎        | 4/30 [00:00<00:02, 11.34 examples/s]\n",
            "\n",
            "   Map:  20%|██        | 6/30 [00:00<00:01, 12.85 examples/s]\n",
            "\n",
            "   Map:  27%|██▋       | 8/30 [00:00<00:01, 12.93 examples/s]\n",
            "\n",
            "   Map:  33%|███▎      | 10/30 [00:00<00:01, 13.44 examples/s]\n",
            "\n",
            "   Map:  40%|████      | 12/30 [00:01<00:01, 10.47 examples/s]\n",
            "\n",
            "   Map:  47%|████▋     | 14/30 [00:01<00:01,  8.59 examples/s]\n",
            "\n",
            "   Map:  53%|█████▎    | 16/30 [00:01<00:01,  8.39 examples/s]\n",
            "\n",
            "   Map:  57%|█████▋    | 17/30 [00:01<00:01,  7.32 examples/s]\n",
            "\n",
            "   Map:  60%|██████    | 18/30 [00:02<00:01,  7.38 examples/s]\n",
            "\n",
            "   Map:  63%|██████▎   | 19/30 [00:02<00:01,  5.64 examples/s]\n",
            "\n",
            "   Map:  67%|██████▋   | 20/30 [00:02<00:01,  5.53 examples/s]\n",
            "\n",
            "   Map:  80%|████████  | 24/30 [00:02<00:00, 10.82 examples/s]\n",
            "\n",
            "   Map:  93%|█████████▎| 28/30 [00:02<00:00, 15.88 examples/s]\n",
            "\n",
            "   Map: 100%|██████████| 30/30 [00:02<00:00, 10.46 examples/s]\n",
            "\n",
            "   max_steps is given, it will override any value given in num_train_epochs\n",
            "\n",
            "   \n",
            "\n",
            "     0%|          | 0/120 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "\n",
            "   To disable this warning, you can either:\n",
            "\n",
            "   \t- Avoid using `tokenizers` before the fork if possible\n",
            "\n",
            "   \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "   Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n",
            "\n",
            "   `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "\n",
            "   /usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "\n",
            "     warnings.warn(\n",
            "\n",
            "   \n",
            "\n",
            "     1%|          | 1/120 [00:13<26:44, 13.48s/it]\n",
            "\n",
            "     2%|▏         | 2/120 [00:25<24:20, 12.38s/it]\n",
            "\n",
            "     2%|▎         | 3/120 [00:41<28:05, 14.41s/it]\n",
            "\n",
            "     3%|▎         | 4/120 [00:55<27:19, 14.13s/it]\n",
            "\n",
            "     4%|▍         | 5/120 [01:09<26:50, 14.00s/it]\n",
            "\n",
            "     5%|▌         | 6/120 [01:24<27:14, 14.34s/it]\n",
            "\n",
            "     6%|▌         | 7/120 [01:35<25:18, 13.44s/it]\n",
            "\n",
            "     7%|▋         | 8/120 [01:48<24:24, 13.08s/it]\n",
            "\n",
            "     8%|▊         | 9/120 [02:05<26:38, 14.40s/it]\n",
            "\n",
            "     8%|▊         | 10/120 [02:20<26:57, 14.70s/it]\n",
            "\n",
            "     9%|▉         | 11/120 [03:05<43:32, 23.97s/it]\n",
            "\n",
            "    10%|█         | 12/120 [04:11<1:05:40, 36.49s/it]\n",
            "\n",
            "    11%|█         | 13/120 [04:42<1:02:06, 34.82s/it]\n",
            "\n",
            "    12%|█▏        | 14/120 [05:46<1:17:13, 43.71s/it]\n",
            "\n",
            "    12%|█▎        | 15/120 [06:20<1:11:40, 40.96s/it]\n",
            "\n",
            "    13%|█▎        | 16/120 [07:28<1:25:09, 49.13s/it]\n",
            "\n",
            "    14%|█▍        | 17/120 [07:54<1:12:14, 42.08s/it]\n",
            "\n",
            "    15%|█▌        | 18/120 [08:43<1:14:58, 44.11s/it]\n",
            "\n",
            "    16%|█▌        | 19/120 [09:26<1:13:30, 43.67s/it]\n",
            "\n",
            "    17%|█▋        | 20/120 [10:57<1:36:28, 57.88s/it]\n",
            "\n",
            "    18%|█▊        | 21/120 [11:00<1:08:21, 41.43s/it]\n",
            "\n",
            "    18%|█▊        | 22/120 [11:03<48:55, 29.96s/it]  \n",
            "\n",
            "    19%|█▉        | 23/120 [11:06<35:30, 21.96s/it]\n",
            "\n",
            "    20%|██        | 24/120 [11:09<26:03, 16.29s/it]\n",
            "\n",
            "    21%|██        | 25/120 [11:13<19:35, 12.38s/it]\n",
            "\n",
            "    22%|██▏       | 26/120 [11:16<15:13,  9.72s/it]\n",
            "\n",
            "    22%|██▎       | 27/120 [11:19<12:01,  7.76s/it]\n",
            "\n",
            "    23%|██▎       | 28/120 [11:23<10:01,  6.54s/it]\n",
            "\n",
            "    24%|██▍       | 29/120 [11:26<08:17,  5.46s/it]\n",
            "\n",
            "    25%|██▌       | 30/120 [11:29<07:08,  4.76s/it]\n",
            "\n",
            "    26%|██▌       | 31/120 [11:41<10:14,  6.90s/it]\n",
            "\n",
            "    27%|██▋       | 32/120 [11:52<12:04,  8.24s/it]\n",
            "\n",
            "    28%|██▊       | 33/120 [12:08<15:20, 10.58s/it]\n",
            "\n",
            "    28%|██▊       | 34/120 [12:22<16:26, 11.48s/it]\n",
            "\n",
            "    29%|██▉       | 35/120 [12:36<17:16, 12.20s/it]\n",
            "\n",
            "    30%|███       | 36/120 [12:51<18:18, 13.08s/it]\n",
            "\n",
            "    31%|███       | 37/120 [13:03<17:35, 12.72s/it]\n",
            "\n",
            "    32%|███▏      | 38/120 [13:15<17:11, 12.58s/it]\n",
            "\n",
            "    32%|███▎      | 39/120 [13:32<18:51, 13.97s/it]\n",
            "\n",
            "    33%|███▎      | 40/120 [13:48<19:09, 14.37s/it]\n",
            "\n",
            "    34%|███▍      | 41/120 [14:33<31:07, 23.64s/it]\n",
            "\n",
            "    35%|███▌      | 42/120 [15:38<46:57, 36.13s/it]\n",
            "\n",
            "    36%|███▌      | 43/120 [16:09<44:20, 34.55s/it]\n",
            "\n",
            "    37%|███▋      | 44/120 [17:13<54:51, 43.31s/it]\n",
            "\n",
            "    38%|███▊      | 45/120 [17:47<50:51, 40.69s/it]\n",
            "\n",
            "    38%|███▊      | 46/120 [18:55<1:00:18, 48.90s/it]\n",
            "\n",
            "    39%|███▉      | 47/120 [19:21<51:03, 41.97s/it]  \n",
            "\n",
            "    40%|████      | 48/120 [20:10<52:44, 43.95s/it]\n",
            "\n",
            "    41%|████      | 49/120 [20:52<51:29, 43.51s/it]\n",
            "\n",
            "    42%|████▏     | 50/120 [22:23<1:07:20, 57.72s/it]\n",
            "\n",
            "    42%|████▎     | 51/120 [22:26<47:31, 41.33s/it]  \n",
            "\n",
            "    43%|████▎     | 52/120 [22:29<33:52, 29.89s/it]\n",
            "\n",
            "    44%|████▍     | 53/120 [22:33<24:29, 21.93s/it]\n",
            "\n",
            "    45%|████▌     | 54/120 [22:36<17:53, 16.26s/it]\n",
            "\n",
            "    46%|████▌     | 55/120 [22:39<13:26, 12.41s/it]\n",
            "\n",
            "    47%|████▋     | 56/120 [22:43<10:23,  9.74s/it]\n",
            "\n",
            "    48%|████▊     | 57/120 [22:46<08:09,  7.77s/it]\n",
            "\n",
            "    48%|████▊     | 58/120 [22:49<06:42,  6.49s/it]\n",
            "\n",
            "    49%|████▉     | 59/120 [22:52<05:31,  5.43s/it]\n",
            "\n",
            "    50%|█████     | 60/120 [22:55<04:44,  4.74s/it]\n",
            "\n",
            "    51%|█████     | 61/120 [23:07<06:45,  6.87s/it]\n",
            "\n",
            "    52%|█████▏    | 62/120 [23:19<07:56,  8.22s/it]\n",
            "\n",
            "    52%|█████▎    | 63/120 [23:35<10:02, 10.56s/it]\n",
            "\n",
            "    53%|█████▎    | 64/120 [23:48<10:44, 11.52s/it]\n",
            "\n",
            "    54%|█████▍    | 65/120 [24:02<11:12, 12.22s/it]\n",
            "\n",
            "    55%|█████▌    | 66/120 [24:17<11:47, 13.09s/it]\n",
            "\n",
            "    56%|█████▌    | 67/120 [24:29<11:11, 12.68s/it]\n",
            "\n",
            "    57%|█████▋    | 68/120 [24:41<10:52, 12.55s/it]\n",
            "\n",
            "    57%|█████▊    | 69/120 [24:59<11:50, 13.94s/it]\n",
            "\n",
            "    58%|█████▊    | 70/120 [25:14<11:56, 14.33s/it]\n",
            "\n",
            "    59%|█████▉    | 71/120 [25:59<19:15, 23.58s/it]\n",
            "\n",
            "    60%|██████    | 72/120 [27:04<28:54, 36.14s/it]\n",
            "\n",
            "    61%|██████    | 73/120 [27:35<27:07, 34.62s/it]\n",
            "\n",
            "    62%|██████▏   | 74/120 [28:39<33:11, 43.28s/it]\n",
            "\n",
            "    62%|██████▎   | 75/120 [29:13<30:28, 40.63s/it]\n",
            "\n",
            "    63%|██████▎   | 76/120 [30:21<35:46, 48.78s/it]\n",
            "\n",
            "    64%|██████▍   | 77/120 [30:47<30:01, 41.89s/it]\n",
            "\n",
            "    65%|██████▌   | 78/120 [31:36<30:44, 43.91s/it]\n",
            "\n",
            "    66%|██████▌   | 79/120 [32:18<29:43, 43.50s/it]\n",
            "\n",
            "    67%|██████▋   | 80/120 [33:49<38:28, 57.72s/it]\n",
            "\n",
            "    68%|██████▊   | 81/120 [33:52<26:51, 41.32s/it]\n",
            "\n",
            "    68%|██████▊   | 82/120 [33:55<18:57, 29.92s/it]\n",
            "\n",
            "    69%|██████▉   | 83/120 [33:59<13:31, 21.92s/it]\n",
            "\n",
            "    70%|███████   | 84/120 [34:02<09:45, 16.26s/it]\n",
            "\n",
            "    71%|███████   | 85/120 [34:05<07:12, 12.35s/it]\n",
            "\n",
            "    72%|███████▏  | 86/120 [34:09<05:30,  9.71s/it]\n",
            "\n",
            "    72%|███████▎  | 87/120 [34:12<04:15,  7.73s/it]\n",
            "\n",
            "    73%|███████▎  | 88/120 [34:15<03:26,  6.47s/it]\n",
            "\n",
            "    74%|███████▍  | 89/120 [34:18<02:47,  5.42s/it]\n",
            "\n",
            "    75%|███████▌  | 90/120 [34:21<02:22,  4.73s/it]\n",
            "\n",
            "    76%|███████▌  | 91/120 [34:33<03:20,  6.91s/it]\n",
            "\n",
            "    77%|███████▋  | 92/120 [34:45<03:50,  8.24s/it]\n",
            "\n",
            "    78%|███████▊  | 93/120 [35:01<04:45, 10.59s/it]\n",
            "\n",
            "    78%|███████▊  | 94/120 [35:14<04:58, 11.48s/it]\n",
            "\n",
            "    79%|███████▉  | 95/120 [35:28<05:05, 12.20s/it]\n",
            "\n",
            "    80%|████████  | 96/120 [35:43<05:13, 13.08s/it]\n",
            "\n",
            "    81%|████████  | 97/120 [35:55<04:51, 12.66s/it]\n",
            "\n",
            "    82%|████████▏ | 98/120 [36:07<04:34, 12.49s/it]\n",
            "\n",
            "    82%|████████▎ | 99/120 [36:24<04:51, 13.89s/it]\n",
            "\n",
            "    83%|████████▎ | 100/120 [36:40<04:46, 14.35s/it]\n",
            "\n",
            "    84%|████████▍ | 101/120 [37:25<07:28, 23.59s/it]\n",
            "\n",
            "    85%|████████▌ | 102/120 [38:30<10:49, 36.09s/it]\n",
            "\n",
            "    86%|████████▌ | 103/120 [39:01<09:46, 34.48s/it]\n",
            "\n",
            "    87%|████████▋ | 104/120 [40:04<11:29, 43.12s/it]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}